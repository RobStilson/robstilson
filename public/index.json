[{"authors":["admin"],"categories":null,"content":"Rob Stilson is a Data Scientist, I-O Psychologist, and Adjunct Professor. The purpose of this site is to blend I-O Psychology, Data Science, and HR together. I anticipate most of these posts will be about People Analytics related topics as I\u0026rsquo;ve had trouble finding a lot of R/Python posts specifically related to HR type work. So, I will put everything that I\u0026rsquo;ve learned over the years into my post and hopefully if you have struggled with data wrangling, data visualization, model building, survey analysis, etc., you can find what you are looking for here.\n","date":1554595200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1554595200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"http://robstilson.rbind.io/author/rob-stilson/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/rob-stilson/","section":"authors","summary":"Rob Stilson is a Data Scientist, I-O Psychologist, and Adjunct Professor. The purpose of this site is to blend I-O Psychology, Data Science, and HR together. I anticipate most of these posts will be about People Analytics related topics as I\u0026rsquo;ve had trouble finding a lot of R/Python posts specifically related to HR type work.","tags":null,"title":"Rob Stilson","type":"authors"},{"authors":["吳恩達"],"categories":null,"content":"Rob Stilson is a Data Scientist, I-O Psychologist, and Adjunct Professor. The purpose of this site is to blend I-O Psychology, Data Science, and HR together. I anticipate most of these posts will be about People Analytics related topics as I\u0026rsquo;ve had trouble finding a lot of R/Python posts specifically related to HR type work. So, I will put everything that I\u0026rsquo;ve learned over the years into my post and hopefully if you have struggled with data wrangling, data visualization, model building, survey analysis, etc., you can find what you are looking for here.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"bb560906b6a99893cc21387348c0b074","permalink":"http://robstilson.rbind.io/author/rob-stilson/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/rob-stilson/","section":"authors","summary":"Rob Stilson is a Data Scientist, I-O Psychologist, and Adjunct Professor. The purpose of this site is to blend I-O Psychology, Data Science, and HR together. I anticipate most of these posts will be about People Analytics related topics as I\u0026rsquo;ve had trouble finding a lot of R/Python posts specifically related to HR type work.","tags":null,"title":"Rob Stilson","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"http://robstilson.rbind.io/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"http://robstilson.rbind.io/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"http://robstilson.rbind.io/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"http://robstilson.rbind.io/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":[],"categories":["Joins"],"content":"\rconflict_prefer(\u0026quot;filter\u0026quot;, \u0026quot;dplyr\u0026quot;)\r## [conflicted] Removing existing preference\r## [conflicted] Will prefer dplyr::filter over any other package\rSummary\rIn the Part 2, we covered loading Excel data from you local machine into R. Now we will cover how to join all of that data together into one worksheet.\nBring in the data\rIf you haven’t look at Part 2 yet, please go there now in order to get the various worksheets we are going to use directly onto your machine.\nNow we will load the data from our desktop. You will need to change the directory to reflect where you stored the file. We will call the data df, which is short for data frame.\nRemember the sheets were:\n\raccident\rfair_pay\rhr\n\rhr_2\n\rperformance\rrecruitment\rsurvey\n\rsurvey_2\r\rWe will lump all the code necessary to get the worksheets onto our machine together and then get to joining them together.\nxl_data \u0026lt;- \u0026quot;D:/Blog/Additional Data for Attrition Data Set_09_09_2020.xlsx\u0026quot;\rtab_names \u0026lt;- excel_sheets(path = xl_data)\rlist_all \u0026lt;- lapply(tab_names, function(x) read_excel(path = xl_data, sheet = x))\rnames(list_all) \u0026lt;- tab_names\rlist2env(list_all, .GlobalEnv)\r## \u0026lt;environment: R_GlobalEnv\u0026gt;\rIn your Global Environment, you should see the following:\n\r8 dfs that we just uploaded\n\r1 “List of 8”\n\r2 Values\n\rtab_names\rxl_data\r\r\rWe’re going to join all of the data from the 8 worksheets together into one larger worksheet that we will call df.\n\r\r","date":1607472000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607560400,"objectID":"fe164819dbfd8f4aeaf06c130066f078","permalink":"http://robstilson.rbind.io/post/working-with-data-part-3-joining-worksheets-together/","publishdate":"2020-12-09T00:00:00Z","relpermalink":"/post/working-with-data-part-3-joining-worksheets-together/","section":"post","summary":"conflict_prefer(\u0026quot;filter\u0026quot;, \u0026quot;dplyr\u0026quot;)\r## [conflicted] Removing existing preference\r## [conflicted] Will prefer dplyr::filter over any other package\rSummary\rIn the Part 2, we covered loading Excel data from you local machine into R.","tags":[],"title":"Working with  Data-Part 3-Joining worksheets together","type":"post"},{"authors":[],"categories":["Joins"],"content":"\r\rSummary\rBring in the data\rDetermining what columns on which to join\r\r\r\rSummary\rIn the Part 2, we covered loading Excel data from you local machine into R. Now we will cover how to join all of that data together into one worksheet.\nBring in the data\rIf you haven’t look at Part 2 yet, please go there now in order to get the various worksheets we are going to use directly onto your machine.\nNow we will load the data from our desktop. You will need to change the directory to reflect where you stored the file. We will call the data df, which is short for data frame.\nRemember the sheets were:\n\raccident\rfair_pay\rhr\n\rhr_2\n\rperformance\rrecruitment\rsurvey\n\rsurvey_2\r\rWe will lump all the code necessary to get the worksheets onto our machine together and then get to joining them together.\nxl_data \u0026lt;- \u0026quot;D:/Blog/Additional Data for Attrition Data Set_09_09_2020.xlsx\u0026quot;\rtab_names \u0026lt;- excel_sheets(path = xl_data)\rlist_all \u0026lt;- lapply(tab_names, function(x) read_excel(path = xl_data, sheet = x))\rnames(list_all) \u0026lt;- tab_names\rlist2env(list_all, .GlobalEnv)\r## \u0026lt;environment: R_GlobalEnv\u0026gt;\rIn your Global Environment, you should see the following:\n\r8 dfs that we just uploaded\n\r1 “List of 8”\n\r2 Values\n\rtab_names\rxl_data\r\r\rWe’re going to join all of the data from the 8 worksheets together into one larger worksheet that we will call df.\n\rDetermining what columns on which to join\rOriginally, I thought this was going to be easy as I assumed (and we know what that does…) that across all 8 dfs, we had a variable that was common to all of them, but that turned out to not be the case.\nSo, we will work backwards and I’ll tell you where we end up.\nI found this excellent answer on Stackoverflow and wanted to include it in this write up as I think you may find additional uses for it.\nThe original question was, “how do I find the common column (or columns) across all of my dfs of interest so that I can then use those to join all of my data together?” It turns out this only works if you actually have a common column across all of your dfs. If employee_id occurs in 99 out of 100 of the files you are trying to merge together, you are now in trouble, because the solutions I found won’t return anything it the column isn’t present in all 100 (or however many) dfs you have.\nBut I found a bit of a work around. If we take the original solution to the question asked, we will assign our list_all that we used to get the tab names from our worksheet and pull all of the data in to L (but it could be any letter or name you choose).\nThen, we will combine the following:\r* table - table uses the cross-classifying factors to build a contingency table of the counts at each combination of factor levels.\r* unlist - Given a list structure x, unlist simplifies it to produce a vector which contains all the atomic components which occur in x.\r* lapply - lapply returns a list of the same length as X, each element of which is the result of applying FUN to the corresponding element of X.\r* names - Functions to get or set the names of an object.\nSo the code tab \u0026lt;- table(unlist(lapply(L, names))) is doing the following:\r1. Looks for the columns names within L or whatever you assigned list_all to\r2. unlist all of them via lapply\r3. Creates a table of the column names across all of the dfs in L and tallies up how many times they occurred\r4. Assigns them to the object tab\nThen, the final line of code, names(tab[tab == length(L)]) looks for any names in the table tab that occur the same amount of times as the length of L, your list, which in this instance is 8.\nIf you are unfamiliar with any of the R functions above, I implore you to learn more about them as you will probably see them a lot.\nAlright, what actually happens if we run the code?\n#From: https://stackoverflow.com/questions/52860105/how-to-find-common-variables-in-different-data-frames\rL \u0026lt;- list_all\rtab \u0026lt;- table(unlist(lapply(L, names)))\rnames(tab[tab == length(L)])\r## character(0)\rcharacter(0). What does that mean? That means that no columns occurred across all of the dfs. You may be feeling despondent at this point, but don’t give up hope! There is magic in that second line tab \u0026lt;- table(unlist(lapply(L, names))) as it will tells us which columns occur and how many times it happens.\nLet’s run the code again, but just the table(unlist(lapply(L, names))) portion to see what we get.\ntable(unlist(lapply(L, names)))\r## ## accident_type attrition department employee_id ## 1 1 3 7 ## engagement gender job_level location ## 2 1 2 1 ## new_hire overtime_hours performance_rating rating ## 1 1 1 1 ## recruiting_source salary sales_quota_pct vacation_days_taken ## 1 2 1 1 ## year ## 3\rThis is very useful information! We can see that accident_type showed up once across all 8 dfs, salary showed up twice and so forth, but the gem is that employee_id showed up 7 times. It is also great that employee_id happens to be a unique identifier so that is what we will use to merge 7 out of our 8 dfs together.\nHere is where we jump the shark…I was wondering, well if I had 100 dfs and only 99 of them had the required column for merging…how would I know which one didn’t. The brute force way is to simply go through each and every one of them and check, but that is not scalable nor conducive to a mistake free process. After searching a bit, I came across (this solution)[https://stackoverflow.com/questions/32251679/extracting-data-frames-from-a-list-based-on-column-names-in-r] on Stackoverflow. We create a function to take a look into all the dfs in our list, list_all and return a logical if employee_id is or is not present. Notice that for recruitment, it list FALSE. Sure enough, if you look at the recruitment df, it does not contain our desired column.\n#From: https://stackoverflow.com/questions/32251679/extracting-data-frames-from-a-list-based-on-column-names-in-r\rreqnames \u0026lt;- c(\u0026quot;employee_id\u0026quot;)\rlapply(list_all, function(x) any(names(x) %in% reqnames))\r## $accident\r## [1] TRUE\r## ## $fair_pay\r## [1] TRUE\r## ## $hr\r## [1] TRUE\r## ## $hr_2\r## [1] TRUE\r## ## $performance\r## [1] TRUE\r## ## $recruitment\r## [1] FALSE\r## ## $survey\r## [1] TRUE\r## ## $survey_2\r## [1] TRUE\rIf we count, we see that recruitment is the 6th df in the list. We can create a new list called new_list where we remove recruitement and then we can run it through our code from above.\nnew_list \u0026lt;- list_all[-6]\rL \u0026lt;- new_list\rtab \u0026lt;- table(unlist(lapply(L, names)))\rnames(tab[tab == length(L)])\r## [1] \u0026quot;employee_id\u0026quot;\rIt now returns employee_id!\n\r\r","date":1607472000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607560400,"objectID":"8b16e51e898d766245ee56867e01f108","permalink":"http://robstilson.rbind.io/post/working-with-data-part-3-joining-worksheets-together/","publishdate":"2020-12-09T00:00:00Z","relpermalink":"/post/working-with-data-part-3-joining-worksheets-together/","section":"post","summary":"Summary\rBring in the data\rDetermining what columns on which to join\r\r\r\rSummary\rIn the Part 2, we covered loading Excel data from you local machine into R.","tags":[],"title":"Working with Data-Part 3-Joining worksheets together","type":"post"},{"authors":[],"categories":["EDA"],"content":"\r\rSummary\rGet Packages\rLoad packages\rBring in the data\rImporting a specific worksheet from Excel\rImporting multiple worksheets from Excel\r\r\r\rSummary\rIn the last post we learned how to get data from GitHub. In this post I will introduce you to getting data from an Excel file that you have on your desktop. Remember from last time we saved a file called “Additional Data for Attrition Data Set_” that included a time stamp created by R. In my case, the full file name was “Additional Data for Attrition Data Set_09_09_2020.xlsx”. Let’s pull it back into R using the readxl package from the tidyverse. Before we begin, remember to get and load the necessary packages.\nGet Packages\rIf you’ve read my post before, this may be repetitive, but this code chunk will load the necessary packages for the analysis. I’ve been using pacman for years and I encourage you to check it out as well. The tryCatch portion of the code will check your machine to see if pacman has already been installed. If it finds pacman, it will download it and if not it will move on. The pacman::p_load function is a wrapper for library and require and checks to see if the listed packages are installed. If they are not, it will attempt to install them from CRAN and/or any other repository in the pacman repository list (See Description under ?p_load for additional information).\nAs a best practice, I also like to give a brief snippet of what each of the packages I’m loading will do in the analysis. This is helpful for anybody else who I send my code to, but also for myself if I need to come back to the code 6 months or more down the line.\n\rLoad packages\rHere I’m explicitly calling the packages via library. Notice that tidylog is at the very end. This is necessary to load after dplyr so that all of the functions from tidylog work correctly. This is used in conjunction with the conflicted package. You can read more about it here.\nconflict_prefer(\u0026quot;filter\u0026quot;, \u0026quot;dplyr\u0026quot;)\r## [conflicted] Removing existing preference\r## [conflicted] Will prefer dplyr::filter over any other package\r\rBring in the data\rNow we will load the data from our desktop. You will need to change the directory to reflect where you stored the file. We will call the data df, which is short for data frame.\nlibrary(readxl)\rdf \u0026lt;- read_excel(\u0026quot;D:/Blog/Additional Data for Attrition Data Set_09_09_2020.xlsx\u0026quot;)\rOk, what happened? You should now see the df object in your Global Environment, but where is the rest of the data? We had 8 individual tabs and it looks like it only took the first one, accident. What if we wanted the performance tab instead?\n\rImporting a specific worksheet from Excel\rIn order to do that we need to specify the sheet as follows:\nlibrary(readxl)\rperformance \u0026lt;- read_excel(\u0026quot;D:/Blog/Additional Data for Attrition Data Set_09_09_2020.xlsx\u0026quot;, sheet = \u0026quot;performance\u0026quot;)\rNow we have the performance data in our Global Environment. We could do this 8 times for each of the sheets, but is there a better way?\n\rImporting multiple worksheets from Excel\rThis portion is from Todd Peterson’s RPubs site where he shows you how to import multiple worksheets using the lapply function.\nFirst, we will save our path as a variable. This is convenient so that we don’t have to write out the path name each time.\nxl_data \u0026lt;- \u0026quot;D:/Blog/Additional Data for Attrition Data Set_09_09_2020.xlsx\u0026quot;\rNow we can feed our xl_data containing the path into the excel_sheets function from readxl.\ntab_names \u0026lt;- excel_sheets(path = xl_data)\rtab_names should appear in your Global Environment under values with the 8 tab names:\n\raccident\rfair_pay\rhr\n\rhr_2\n\rperformance\rrecruitment\rsurvey\n\rsurvey_2\r\rWe can see all of the names more easily by simply running tab_names.\ntab_names\r## [1] \u0026quot;accident\u0026quot; \u0026quot;fair_pay\u0026quot; \u0026quot;hr\u0026quot; \u0026quot;hr_2\u0026quot; \u0026quot;performance\u0026quot;\r## [6] \u0026quot;recruitment\u0026quot; \u0026quot;survey\u0026quot; \u0026quot;survey_2\u0026quot;\rIf you wanted to remove the quotation marks, you could wrap tab_names in the cat() function.\ncat(tab_names)\r## accident fair_pay hr hr_2 performance recruitment survey survey_2\rLet’s say the data you are working with has a large number of tabs. You could wrap tab_names in cat() and use a separator in order to see them all in a vertical fashion.\ncat(tab_names, sep = \u0026quot;\\n\u0026quot;)\r## accident\r## fair_pay\r## hr\r## hr_2\r## performance\r## recruitment\r## survey\r## survey_2\rThe \"\\n\" in the previous code signifies a line break.\nYou can also place additional characters, like a comma, before the slash to have a vertical list with a comma at the end.\ncat(tab_names, sep = \u0026quot;,\\n\u0026quot;)\r## accident,\r## fair_pay,\r## hr,\r## hr_2,\r## performance,\r## recruitment,\r## survey,\r## survey_2\rOk, back to actually importing the data. Again, we will use the lapply function which you can read more about here in the famous “Using apply, sapply, lapply in R” blog post.\ntab_names \u0026lt;- excel_sheets(path = xl_data)\rlist_all \u0026lt;- lapply(tab_names, function(x) read_excel(path = xl_data, sheet = x))\rstr(list_all)\r## List of 8\r## $ : tibble [302 x 3] (S3: tbl_df/tbl/data.frame)\r## ..$ year : num [1:302] 2017 2017 2017 2017 2017 ...\r## ..$ employee_id : num [1:302] 1 4 11 19 22 23 24 38 41 47 ...\r## ..$ accident_type: chr [1:302] \u0026quot;Mild\u0026quot; \u0026quot;Mild\u0026quot; \u0026quot;Mild\u0026quot; \u0026quot;Mild\u0026quot; ...\r## $ : tibble [1,470 x 5] (S3: tbl_df/tbl/data.frame)\r## ..$ employee_id: num [1:1470] 1 2 4 5 7 8 10 11 12 13 ...\r## ..$ department : chr [1:1470] \u0026quot;Sales\u0026quot; \u0026quot;Engineering\u0026quot; \u0026quot;Engineering\u0026quot; \u0026quot;Engineering\u0026quot; ...\r## ..$ salary : num [1:1470] 103264 80709 60737 99116 51022 ...\r## ..$ new_hire : chr [1:1470] \u0026quot;No\u0026quot; \u0026quot;No\u0026quot; \u0026quot;Yes\u0026quot; \u0026quot;Yes\u0026quot; ...\r## ..$ job_level : chr [1:1470] \u0026quot;Salaried\u0026quot; \u0026quot;Hourly\u0026quot; \u0026quot;Hourly\u0026quot; \u0026quot;Salaried\u0026quot; ...\r## $ : tibble [1,470 x 4] (S3: tbl_df/tbl/data.frame)\r## ..$ employee_id: num [1:1470] 1 2 4 5 7 8 10 11 12 13 ...\r## ..$ department : chr [1:1470] \u0026quot;Sales\u0026quot; \u0026quot;Engineering\u0026quot; \u0026quot;Engineering\u0026quot; \u0026quot;Engineering\u0026quot; ...\r## ..$ job_level : chr [1:1470] \u0026quot;Salaried\u0026quot; \u0026quot;Hourly\u0026quot; \u0026quot;Hourly\u0026quot; \u0026quot;Salaried\u0026quot; ...\r## ..$ gender : chr [1:1470] \u0026quot;Female\u0026quot; \u0026quot;Female\u0026quot; \u0026quot;Female\u0026quot; \u0026quot;Male\u0026quot; ...\r## $ : tibble [2,940 x 4] (S3: tbl_df/tbl/data.frame)\r## ..$ year : num [1:2940] 2016 2017 2016 2017 2016 ...\r## ..$ employee_id : num [1:2940] 1 1 2 2 4 4 5 5 7 7 ...\r## ..$ location : chr [1:2940] \u0026quot;Northwood\u0026quot; \u0026quot;Northwood\u0026quot; \u0026quot;East Valley\u0026quot; \u0026quot;East Valley\u0026quot; ...\r## ..$ overtime_hours: num [1:2940] 14 8 8 11 4 2 0 17 21 9 ...\r## $ : tibble [1,470 x 2] (S3: tbl_df/tbl/data.frame)\r## ..$ employee_id: num [1:1470] 1 2 4 5 7 8 10 11 12 13 ...\r## ..$ rating : num [1:1470] 4 4 4 4 2 5 3 4 3 3 ...\r## $ : tibble [446 x 4] (S3: tbl_df/tbl/data.frame)\r## ..$ attrition : num [1:446] 1 0 1 0 0 1 1 0 0 0 ...\r## ..$ performance_rating: num [1:446] 3 3 2 2 3 3 3 2 3 3 ...\r## ..$ sales_quota_pct : num [1:446] 1.088 2.394 0.498 2.514 1.425 ...\r## ..$ recruiting_source : chr [1:446] \u0026quot;Applied Online\u0026quot; NA \u0026quot;Campus\u0026quot; NA ...\r## $ : tibble [1,470 x 5] (S3: tbl_df/tbl/data.frame)\r## ..$ employee_id : num [1:1470] 1 2 4 5 7 8 10 11 12 13 ...\r## ..$ department : chr [1:1470] \u0026quot;Sales\u0026quot; \u0026quot;Engineering\u0026quot; \u0026quot;Engineering\u0026quot; \u0026quot;Engineering\u0026quot; ...\r## ..$ engagement : num [1:1470] 3 3 3 3 3 3 3 1 4 2 ...\r## ..$ salary : num [1:1470] 103264 80709 60737 99116 51022 ...\r## ..$ vacation_days_taken: num [1:1470] 7 12 12 7 18 9 18 4 12 14 ...\r## $ : tibble [2,940 x 3] (S3: tbl_df/tbl/data.frame)\r## ..$ year : num [1:2940] 2016 2017 2016 2017 2016 ...\r## ..$ employee_id: num [1:2940] 1 1 2 2 4 4 5 5 7 7 ...\r## ..$ engagement : num [1:2940] 3 3 3 3 3 3 3 3 3 3 ...\rAdmittedly, I stumbled during this next step. I wanted to get all of the individual data frames back into the environment as they were when we originally pulled them from GitHub. We’ve got the list now with list_all, but how do we actually access the dfs within? Stackoverflow to rescue! The linked post lays out exactly what I wanted to do and we will use that code below.\nName all of the data frames using our tab_names object.\nnames(list_all) \u0026lt;- tab_names\rThen use the until now unknown to me function of list2env to put them all in the Global Environment. Viola!\nlist2env(list_all, .GlobalEnv)\r## \u0026lt;environment: R_GlobalEnv\u0026gt;\r\r\r","date":1600300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600696725,"objectID":"b344ce8b3e1717a93e387cb939331d74","permalink":"http://robstilson.rbind.io/post/working-with-data-part-2-pulling-in-data-from-excel/","publishdate":"2020-09-17T00:00:00Z","relpermalink":"/post/working-with-data-part-2-pulling-in-data-from-excel/","section":"post","summary":"Summary\rGet Packages\rLoad packages\rBring in the data\rImporting a specific worksheet from Excel\rImporting multiple worksheets from Excel\r\r\r\rSummary\rIn the last post we learned how to get data from GitHub.","tags":[],"title":"Working with Data-Part 2-Pulling in data from Excel","type":"post"},{"authors":[],"categories":["Joins"],"content":"\r\rSummary\rGet Packages\rLoad packages\rBring in the data\rStructure of GitHub pull\rTake a look at the data\rSkimr::skim\rjanitor::tabyl\r\r\rBring in more data\rExporting the data to Excel with openxlsx\r\rWrap up\rJust the code\r\r\rSummary\rIn this post I will introduce you to getting data from existing GitHub sites. We will use data sets found on Dragana Pavolovic’s GitHub page under her Data_analysis repository. I discovered this from one of her blog post and it accentuates the HR data from the IBM Watson Analytics Lab.\nFirst in Part 1, we will download the data into Excel and save it for posterity using the openxlsx package. We will give each data frame its own tab or worksheet in Excel so that it is all in one place. Then, in Part 2 we will show how we upload the data to give you a feel for how you would could bring data into R from an Excel workbook that has multiple worksheets. In Part 3, we will join all of the worksheets together in R using joins from dplyr. Part 4, will consists of some additional EDA with all of the data joined together. We will focus on wide and tall data in Part 5 as we take the data from a single row per person to multiple rows per person as we tidy the data. Part 6 will show off some of the advantages of having your data in tall form. Part 7 will introduce the ggplot2 package for data visualization.\nGet Packages\rIf you’ve read my post before, this may be repetitive, but this code chunk will load the necessary packages for the analysis. I’ve been using pacman for years and I encourage you to check it out as well. The tryCatch portion of the code will check your machine to see if pacman has already been installed. If it finds pacman, it will download it and if not it will move on. The pacman::p_load function is a wrapper for library and require and checks to see if the listed packages are installed. If they are not, it will attempt to install them from CRAN and/or any other repository in the pacman repository list (See Description under ?p_load for additional information).\nAs a best practice, I also like to give a brief snippet of what each of the packages I’m loading will do in the analysis. This is helpful for anybody else who I send my code to, but also for myself if I need to come back to the code 6 months or more down the line.\n\rLoad packages\rHere I’m explicitly calling the packages via library. Notice that tidylog is at the very end. This is necessary to load after dplyr so that all of the functions from tidylog work correctly. This is used in conjunction with the conflicted package. You can read more about it here.\nconflict_prefer(\u0026quot;filter\u0026quot;, \u0026quot;dplyr\u0026quot;)\r## [conflicted] Removing existing preference\r## [conflicted] Will prefer dplyr::filter over any other package\r\rBring in the data\rNow we will grab the data off of GitHub. We will bring it in alphabetically for no particular reason but to help us keep track of what we have and haven’t downloaded. While you are here, I encourage you to check out Dragana’s excellent write up of Human Resource Analytics here\nlibrary(RCurl)\rx \u0026lt;- getURL(\u0026quot;https://raw.githubusercontent.com/Dragana236/Data_analysis/master/accident_data.csv\u0026quot;)\raccident \u0026lt;- read.csv(text = x)\r\rStructure of GitHub pull\rLet’s quickly break down what just happened in case you would also like to get some data off of GitHub that is from a different site.\nFirst, we have https://raw.githubusercontent.com. This is important! You can’t just feed in the regular URL as if you are looking at the data on her site. In that case it would be github.com/Dragana236/Data_analysis. So remember to use https://raw.githubusercontent.com before you go to double face palm land out of frustration. The next portion of the code is straight forward. Simply /Dragana236. This will be the name of the person’s extension on GitHub. Mine is simply /RobStilson. After that, we need to put in the specific repository on GitHub, so in this case /Data_analysis. If the name of the repository was Super_Fun_Data, then you would put in /Super_Fun_Data here. After the repository we specify master. Don’t worry too much about that right now. Finally, we put in the actual name of the data we’re looking for. In this instance, it is accident_data.csv. This part will be the only portion of the code that changes as we work through and download all of the relevant data from this GitHub account.\n\rTake a look at the data\rSkimr::skim\rWe won’t go too deep into this as we covered a lot of this in a previous EDA post. We’ll use the handy skimr package again for the skim function.\nlibrary(skimr)\rskim(accident)\r\rTable 1: Data summary\r\rName\raccident\r\rNumber of rows\r302\r\rNumber of columns\r3\r\r_______________________\r\r\rColumn type frequency:\r\r\rcharacter\r1\r\rnumeric\r2\r\r________________________\r\r\rGroup variables\rNone\r\r\r\rVariable type: character\n\r\rskim_variable\rn_missing\rcomplete_rate\rmin\rmax\rempty\rn_unique\rwhitespace\r\r\r\raccident_type\r0\r1\r4\r8\r0\r3\r0\r\r\r\rVariable type: numeric\n\r\rskim_variable\rn_missing\rcomplete_rate\rmean\rsd\rp0\rp25\rp50\rp75\rp100\rhist\r\r\r\ryear\r0\r1\r2016.59\r0.49\r2016\r2016\r2017.0\r2017.00\r2017\r▆▁▁▁▇\r\remployee_id\r0\r1\r1008.07\r602.35\r1\r494\r1001.5\r1510.75\r2064\r▇▆▇▆▆\r\r\r\rHooray, we have an employee_id variable! This will make joining the data to our main attrition data set much easier. We also have another numeric column called year and a character column called accident_type.\n\rjanitor::tabyl\rSince we have a character column type, we’ll take an additional peek with the janitor::tabyl function. Remember to use your $ to call the accident_type column.\nlibrary(janitor)\rtabyl(accident$accident_type)\r## accident$accident_type n percent\r## Mild 223 0.73841060\r## Moderate 68 0.22516556\r## Severe 11 0.03642384\rThis table shows us that we have 3 accident types, Mild, Moderate, and Severe and an n count and percent for each.\n\r\r\rBring in more data\rlibrary(RCurl)\rx \u0026lt;- getURL(\u0026quot;https://raw.githubusercontent.com/Dragana236/Data_analysis/master/fair_pay.csv\u0026quot;)\rfair_pay \u0026lt;- read.csv(text = x)\rx \u0026lt;- getURL(\u0026quot;https://raw.githubusercontent.com/Dragana236/Data_analysis/master/hr_data.csv\u0026quot;)\rhr \u0026lt;- read.csv(text = x)\rx \u0026lt;- getURL(\u0026quot;https://raw.githubusercontent.com/Dragana236/Data_analysis/master/hr_2.csv\u0026quot;)\rhr_2 \u0026lt;- read.csv(text = x)\rx \u0026lt;- getURL(\u0026quot;https://raw.githubusercontent.com/Dragana236/Data_analysis/master/performance_data.csv\u0026quot;)\rperformance \u0026lt;- read.csv(text = x)\rx \u0026lt;- getURL(\u0026quot;https://raw.githubusercontent.com/Dragana236/Data_analysis/master/recruitment_data.csv\u0026quot;)\rrecruitment \u0026lt;- read.csv(text = x)\rx \u0026lt;- getURL(\u0026quot;https://raw.githubusercontent.com/Dragana236/Data_analysis/master/survey_data.csv\u0026quot;)\rsurvey \u0026lt;- read.csv(text = x)\rx \u0026lt;- getURL(\u0026quot;https://raw.githubusercontent.com/Dragana236/Data_analysis/master/survey_2.csv\u0026quot;)\rsurvey_2 \u0026lt;- read.csv(text = x)\rWe could take a look at all of this additional data in R, but let’s say you would like to show this to your boss or colleagues who are not R users(…yet). It is more likely that they will have something like Excel, Google Sheets, etc. For now, we will stick with Excel, but we may return to Google Sheets in a future post. Anyway, how do we get our newly found data from R into Excel? Enter the openxlsx package! I really like the openxlsx package because it makes getting the data from R into Excel really easy and it doesn’t have any Java dependencies which can prove problematic (at least for me…). We won’t go too in depth into the openxlsx package here, but there are many great tutorials available and I encourage you to look around for them if you are interested.\nExporting the data to Excel with openxlsx\rTo export the data to Excel, we will create a workbook and then using styles, headers, etc., we’ll make the data pretty and then also put each df into its own worksheet.\nwb \u0026lt;- openxlsx::createWorkbook() #Create a work book\r#########################\r# accident #\r#########################\raddWorksheet(wb, \u0026quot;accident\u0026quot;) #name the worksheet in Excel. This does not have to be the same as the df going into it.\rwriteData(wb, \u0026quot;accident\u0026quot;, accident) # write the `accident` data into your worksheet \u0026quot;accident\u0026quot;.\r#Create style\rstyle1 \u0026lt;- createStyle(fontColour = \u0026quot;#000080\u0026quot;, textDecoration = \u0026quot;Bold\u0026quot;) #Choose your custom font color (https://www.rgbtohex.net/) and make it bold. Call it style1\raddStyle(wb, style = style1, rows = 1, cols = 1:3, sheet = \u0026quot;accident\u0026quot;) #add this style to your worksheet. Tell it which rows and columns\r#Freeze Panes (your co-workers will love you!)\r#Also check here: https://stackoverflow.com/questions/37677326/applying-style-to-all-sheets-of-a-workbook-using-openxlsx-package-in-r\rfreezePane(wb, \u0026quot;accident\u0026quot;, firstActiveRow = 2) #Freeze those panes. You know you want to. Tell it where to start.\r#Add filter\raddFilter(wb, \u0026quot;accident\u0026quot;, row = 1, cols = 1:3) #Add your filter as well. If you\u0026#39;re trying to impress, you might as well go all in :)\r#########################\r# fair_pay #\r#########################\raddWorksheet(wb, \u0026quot;fair_pay\u0026quot;) #name the worksheet in Excel. This does not have to be the same as the df going into it.\rwriteData(wb, \u0026quot;fair_pay\u0026quot;, fair_pay) # write the `fair_pay` data into your worksheet \u0026quot;fair_pay\u0026quot;.\r#Create style\rstyle1 \u0026lt;- createStyle(fontColour = \u0026quot;#000080\u0026quot;, textDecoration = \u0026quot;Bold\u0026quot;) #Choose your custom font color (https://www.rgbtohex.net/) and make it bold. Call it style1\raddStyle(wb, style = style1, rows = 1, cols = 1:5, sheet = \u0026quot;fair_pay\u0026quot;) #add this style to your worksheet. Tell it which rows and columns\r#Freeze Panes (your co-workers will love you!)\r#Also check here: https://stackoverflow.com/questions/37677326/applying-style-to-all-sheets-of-a-workbook-using-openxlsx-package-in-r\rfreezePane(wb, \u0026quot;fair_pay\u0026quot;, firstActiveRow = 2) #Freeze those panes. You know you want to. Tell it where to start.\r#Add filter\raddFilter(wb, \u0026quot;fair_pay\u0026quot;, row = 1, cols = 1:5) #Add your filter as well. If you\u0026#39;re trying to impress, you might as well go all in :)\r#########################\r# hr #\r#########################\raddWorksheet(wb, \u0026quot;hr\u0026quot;) #name the worksheet in Excel. This does not have to be the same as the df going into it.\rwriteData(wb, \u0026quot;hr\u0026quot;, hr) # write the `hr` data into your worksheet \u0026quot;hr\u0026quot;.\r#Create style\rstyle1 \u0026lt;- createStyle(fontColour = \u0026quot;#000080\u0026quot;, textDecoration = \u0026quot;Bold\u0026quot;) #Choose your custom font color (https://www.rgbtohex.net/) and make it bold. Call it style1\raddStyle(wb, style = style1, rows = 1, cols = 1:4, sheet = \u0026quot;hr\u0026quot;) #add this style to your worksheet. Tell it which rows and columns\r#Freeze Panes (your co-workers will love you!)\r#Also check here: https://stackoverflow.com/questions/37677326/applying-style-to-all-sheets-of-a-workbook-using-openxlsx-package-in-r\rfreezePane(wb, \u0026quot;hr\u0026quot;, firstActiveRow = 2) #Freeze those panes. You know you want to. Tell it where to start.\r#Add filter\raddFilter(wb, \u0026quot;hr\u0026quot;, row = 1, cols = 1:4) #Add your filter as well. If you\u0026#39;re trying to impress, you might as well go all in :)\r#########################\r# hr_2 #\r#########################\raddWorksheet(wb, \u0026quot;hr_2\u0026quot;) #name the worksheet in Excel. This does not have to be the same as the df going into it.\rwriteData(wb, \u0026quot;hr_2\u0026quot;, hr_2) # write the `hr_2` data into your worksheet \u0026quot;hr_2\u0026quot;.\r#Create style\rstyle1 \u0026lt;- createStyle(fontColour = \u0026quot;#000080\u0026quot;, textDecoration = \u0026quot;Bold\u0026quot;) #Choose your custom font color (https://www.rgbtohex.net/) and make it bold. Call it style1\raddStyle(wb, style = style1, rows = 1, cols = 1:4, sheet = \u0026quot;hr_2\u0026quot;) #add this style to your worksheet. Tell it which rows and columns\r#Freeze Panes (your co-workers will love you!)\r#Also check here: https://stackoverflow.com/questions/37677326/applying-style-to-all-sheets-of-a-workbook-using-openxlsx-package-in-r\rfreezePane(wb, \u0026quot;hr_2\u0026quot;, firstActiveRow = 2) #Freeze those panes. You know you want to. Tell it where to start.\r#Add filter\raddFilter(wb, \u0026quot;hr_2\u0026quot;, row = 1, cols = 1:4) #Add your filter as well. If you\u0026#39;re trying to impress, you might as well go all in :)\r#########################\r# performance #\r#########################\raddWorksheet(wb, \u0026quot;performance\u0026quot;) #name the worksheet in Excel. This does not have to be the same as the df going into it.\rwriteData(wb, \u0026quot;performance\u0026quot;, performance) # write the `performance` data into your worksheet \u0026quot;performance\u0026quot;.\r#Create style\rstyle1 \u0026lt;- createStyle(fontColour = \u0026quot;#000080\u0026quot;, textDecoration = \u0026quot;Bold\u0026quot;) #Choose your custom font color (https://www.rgbtohex.net/) and make it bold. Call it style1\raddStyle(wb, style = style1, rows = 1, cols = 1:2, sheet = \u0026quot;performance\u0026quot;) #add this style to your worksheet. Tell it which rows and columns\r#Freeze Panes (your co-workers will love you!)\r#Also check here: https://stackoverflow.com/questions/37677326/applying-style-to-all-sheets-of-a-workbook-using-openxlsx-package-in-r\rfreezePane(wb, \u0026quot;performance\u0026quot;, firstActiveRow = 2) #Freeze those panes. You know you want to. Tell it where to start.\r#Add filter\raddFilter(wb, \u0026quot;performance\u0026quot;, row = 1, cols = 1:2) #Add your filter as well. If you\u0026#39;re trying to impress, you might as well go all in :)\r#########################\r# recruitment #\r#########################\raddWorksheet(wb, \u0026quot;recruitment\u0026quot;) #name the worksheet in Excel. This does not have to be the same as the df going into it.\rwriteData(wb, \u0026quot;recruitment\u0026quot;, recruitment) # write the `recruitment` data into your worksheet \u0026quot;recruitment\u0026quot;.\r#Create style\rstyle1 \u0026lt;- createStyle(fontColour = \u0026quot;#000080\u0026quot;, textDecoration = \u0026quot;Bold\u0026quot;) #Choose your custom font color (https://www.rgbtohex.net/) and make it bold. Call it style1\raddStyle(wb, style = style1, rows = 1, cols = 1:4, sheet = \u0026quot;recruitment\u0026quot;) #add this style to your worksheet. Tell it which rows and columns\r#Freeze Panes (your co-workers will love you!)\r#Also check here: https://stackoverflow.com/questions/37677326/applying-style-to-all-sheets-of-a-workbook-using-openxlsx-package-in-r\rfreezePane(wb, \u0026quot;recruitment\u0026quot;, firstActiveRow = 2) #Freeze those panes. You know you want to. Tell it where to start.\r#Add filter\raddFilter(wb, \u0026quot;recruitment\u0026quot;, row = 1, cols = 1:4) #Add your filter as well. If you\u0026#39;re trying to impress, you might as well go all in :)\r#########################\r# survey #\r#########################\raddWorksheet(wb, \u0026quot;survey\u0026quot;) #name the worksheet in Excel. This does not have to be the same as the df going into it.\rwriteData(wb, \u0026quot;survey\u0026quot;, survey) # write the `survey` data into your worksheet \u0026quot;survey\u0026quot;.\r#Create style\rstyle1 \u0026lt;- createStyle(fontColour = \u0026quot;#000080\u0026quot;, textDecoration = \u0026quot;Bold\u0026quot;) #Choose your custom font color (https://www.rgbtohex.net/) and make it bold. Call it style1\raddStyle(wb, style = style1, rows = 1, cols = 1:5, sheet = \u0026quot;survey\u0026quot;) #add this style to your worksheet. Tell it which rows and columns\r#Freeze Panes (your co-workers will love you!)\r#Also check here: https://stackoverflow.com/questions/37677326/applying-style-to-all-sheets-of-a-workbook-using-openxlsx-package-in-r\rfreezePane(wb, \u0026quot;survey\u0026quot;, firstActiveRow = 2) #Freeze those panes. You know you want to. Tell it where to start.\r#Add filter\raddFilter(wb, \u0026quot;survey\u0026quot;, row = 1, cols = 1:5) #Add your filter as well. If you\u0026#39;re trying to impress, you might as well go all in :)\r#########################\r# survey_2 #\r#########################\raddWorksheet(wb, \u0026quot;survey_2\u0026quot;) #name the worksheet in Excel. This does not have to be the same as the df going into it.\rwriteData(wb, \u0026quot;survey_2\u0026quot;, survey_2) # write the `survey_2` data into your worksheet \u0026quot;survey_2\u0026quot;.\r#Create style\rstyle1 \u0026lt;- createStyle(fontColour = \u0026quot;#000080\u0026quot;, textDecoration = \u0026quot;Bold\u0026quot;) #Choose your custom font color (https://www.rgbtohex.net/) and make it bold. Call it style1\raddStyle(wb, style = style1, rows = 1, cols = 1:3, sheet = \u0026quot;survey_2\u0026quot;) #add this style to your worksheet. Tell it which rows and columns\r#Freeze Panes (your co-workers will love you!)\r#Also check here: https://stackoverflow.com/questions/37677326/applying-style-to-all-sheets-of-a-workbook-using-openxlsx-package-in-r\rfreezePane(wb, \u0026quot;survey_2\u0026quot;, firstActiveRow = 2) #Freeze those panes. You know you want to. Tell it where to start.\r#Add filter\raddFilter(wb, \u0026quot;survey_2\u0026quot;, row = 1, cols = 1:3) #Add your filter as well. If you\u0026#39;re trying to impress, you might as well go all in :)\r#Now we\u0026#39;ll do a fancy save by customizing the file name using paste0 and system time. We\u0026#39;ll also assume this was for the previous month. You\u0026#39;ll also need to make this path the one you want on your computer. saveWorkbook(wb, paste0(\u0026quot;00_Data/Additional Data for Attrition Data Set_\u0026quot;, format(Sys.Date(), \u0026quot;%d_%m_%Y\u0026quot;) , \u0026quot;.xlsx\u0026quot;), overwrite = TRUE)\r\r\rWrap up\rYou have now downloaded the data off of GitHub and put each df into its own worksheet in Excel, made the column names bold, added a custom color, frozen the panes, added a filter, and named the file with a data extension!\nI hope you have enjoyed reading this and working through this on your own and now feel like you can do this with your own projects. Please join us for the subsequent parts as we continue to build out and explore this new data along with the original attrition data set.\n\rJust the code\r#\u0026#39; \u0026lt;!-- ####################################################################################################### --\u0026gt;\r#\u0026#39; \u0026lt;!-- ####################################################################################################### --\u0026gt;\r#\u0026#39; \u0026lt;!-- ##################################LOADING PACKAGES##################################################### --\u0026gt;\rtryCatch(require(pacman),finally=utils:::install.packages(pkgs=\u0026#39;pacman\u0026#39;,repos=\u0026#39;http://cran.r-project.org\u0026#39;));\rrequire(pacman)\r# if the above doesn\u0026#39;t work, use this code#\r# tryCatch\r# detach(\u0026quot;package:pacman\u0026quot;, unload = TRUE)\r# install.packages(\u0026quot;pacman\u0026quot;, dependencies = TRUE)\r# install.packages(\u0026quot;pacman\u0026quot;)\rpacman::p_load(modeldata, # data sets useful for modeling packages\rdplyr, # a grammar of data manipulation\rtidyr, # tidy messy data\rreadxl, # read Excel files\rggplot2, # create elegant data visualisations using the grammar of graphics\rknitr, # a general-purpose package for dynamic report generation in R\rbroom, # convert statistical objects into tidy tibbles\rpurrr, # functional programming tools\rpsych, #procedures for psychological, psychometric, and personality research\rconflicted, # an alternative conflict resolution strategy for like named function in different libraries\rjanitor, # simple tools for examining and cleaning dirty data\rskimr, # compact and flexible summaries of data\ropenxlsx, # read write and edit xlsx files\rRCurl, # general Network (HTTP/FTP/...) Client Interface for R\rtidyquant, # tidy quantitative financial analysis\rtidylog # logging for `dplyr` and `tidyr` functions\r)\r#\u0026#39; \u0026lt;!-- #Loading libraries --\u0026gt;\rsuppressPackageStartupMessages({\rlibrary(modeldata) library(dplyr)\rlibrary(tidyr)\rlibrary(readxl)\rlibrary(ggplot2)\rlibrary(knitr)\rlibrary(broom)\rlibrary(purrr)\rlibrary(psych)\rlibrary(conflicted)\rlibrary(janitor)\rlibrary(skimr)\rlibrary(openxlsx)\rlibrary(RCurl)\rlibrary(tidyquant)\rlibrary(tidylog, warn.conflicts = FALSE)\r})\rfor (f in getNamespaceExports(\u0026quot;tidylog\u0026quot;)) {\rconflicted::conflict_prefer(f, \u0026quot;tidylog\u0026quot;, quiet = TRUE)\r}\rconflict_prefer(\u0026quot;filter\u0026quot;, \u0026quot;dplyr\u0026quot;)\rlibrary(RCurl)\rx \u0026lt;- getURL(\u0026quot;https://raw.githubusercontent.com/Dragana236/Data_analysis/master/accident_data.csv\u0026quot;)\raccident \u0026lt;- read.csv(text = x)\rlibrary(skimr)\rskim(accident)\rlibrary(janitor)\rtabyl(accident$accident_type)\rlibrary(RCurl)\rx \u0026lt;- getURL(\u0026quot;https://raw.githubusercontent.com/Dragana236/Data_analysis/master/fair_pay.csv\u0026quot;)\rfair_pay \u0026lt;- read.csv(text = x)\rx \u0026lt;- getURL(\u0026quot;https://raw.githubusercontent.com/Dragana236/Data_analysis/master/hr_data.csv\u0026quot;)\rhr \u0026lt;- read.csv(text = x)\rx \u0026lt;- getURL(\u0026quot;https://raw.githubusercontent.com/Dragana236/Data_analysis/master/hr_2.csv\u0026quot;)\rhr_2 \u0026lt;- read.csv(text = x)\rx \u0026lt;- getURL(\u0026quot;https://raw.githubusercontent.com/Dragana236/Data_analysis/master/performance_data.csv\u0026quot;)\rperformance \u0026lt;- read.csv(text = x)\rx \u0026lt;- getURL(\u0026quot;https://raw.githubusercontent.com/Dragana236/Data_analysis/master/recruitment_data.csv\u0026quot;)\rrecruitment \u0026lt;- read.csv(text = x)\rx \u0026lt;- getURL(\u0026quot;https://raw.githubusercontent.com/Dragana236/Data_analysis/master/survey_data.csv\u0026quot;)\rsurvey \u0026lt;- read.csv(text = x)\rx \u0026lt;- getURL(\u0026quot;https://raw.githubusercontent.com/Dragana236/Data_analysis/master/survey_2.csv\u0026quot;)\rsurvey_2 \u0026lt;- read.csv(text = x)\rwb \u0026lt;- openxlsx::createWorkbook() #Create a work book\r#########################\r# accident #\r#########################\raddWorksheet(wb, \u0026quot;accident\u0026quot;) #name the worksheet in Excel. This does not have to be the same as the df going into it.\rwriteData(wb, \u0026quot;accident\u0026quot;, accident) # write the `accident` data into your worksheet \u0026quot;accident\u0026quot;.\r#Create style\rstyle1 \u0026lt;- createStyle(fontColour = \u0026quot;#000080\u0026quot;, textDecoration = \u0026quot;Bold\u0026quot;) #Choose your custom font color (https://www.rgbtohex.net/) and make it bold. Call it style1\raddStyle(wb, style = style1, rows = 1, cols = 1:3, sheet = \u0026quot;accident\u0026quot;) #add this style to your worksheet. Tell it which rows and columns\r#Freeze Panes (your co-workers will love you!)\r#Also check here: https://stackoverflow.com/questions/37677326/applying-style-to-all-sheets-of-a-workbook-using-openxlsx-package-in-r\rfreezePane(wb, \u0026quot;accident\u0026quot;, firstActiveRow = 2) #Freeze those panes. You know you want to. Tell it where to start.\r#Add filter\raddFilter(wb, \u0026quot;accident\u0026quot;, row = 1, cols = 1:3) #Add your filter as well. If you\u0026#39;re trying to impress, you might as well go all in :)\r#########################\r# fair_pay #\r#########################\raddWorksheet(wb, \u0026quot;fair_pay\u0026quot;) #name the worksheet in Excel. This does not have to be the same as the df going into it.\rwriteData(wb, \u0026quot;fair_pay\u0026quot;, fair_pay) # write the `fair_pay` data into your worksheet \u0026quot;fair_pay\u0026quot;.\r#Create style\rstyle1 \u0026lt;- createStyle(fontColour = \u0026quot;#000080\u0026quot;, textDecoration = \u0026quot;Bold\u0026quot;) #Choose your custom font color (https://www.rgbtohex.net/) and make it bold. Call it style1\raddStyle(wb, style = style1, rows = 1, cols = 1:5, sheet = \u0026quot;fair_pay\u0026quot;) #add this style to your worksheet. Tell it which rows and columns\r#Freeze Panes (your co-workers will love you!)\r#Also check here: https://stackoverflow.com/questions/37677326/applying-style-to-all-sheets-of-a-workbook-using-openxlsx-package-in-r\rfreezePane(wb, \u0026quot;fair_pay\u0026quot;, firstActiveRow = 2) #Freeze those panes. You know you want to. Tell it where to start.\r#Add filter\raddFilter(wb, \u0026quot;fair_pay\u0026quot;, row = 1, cols = 1:5) #Add your filter as well. If you\u0026#39;re trying to impress, you might as well go all in :)\r#########################\r# hr #\r#########################\raddWorksheet(wb, \u0026quot;hr\u0026quot;) #name the worksheet in Excel. This does not have to be the same as the df going into it.\rwriteData(wb, \u0026quot;hr\u0026quot;, hr) # write the `hr` data into your worksheet \u0026quot;hr\u0026quot;.\r#Create style\rstyle1 \u0026lt;- createStyle(fontColour = \u0026quot;#000080\u0026quot;, textDecoration = \u0026quot;Bold\u0026quot;) #Choose your custom font color (https://www.rgbtohex.net/) and make it bold. Call it style1\raddStyle(wb, style = style1, rows = 1, cols = 1:4, sheet = \u0026quot;hr\u0026quot;) #add this style to your worksheet. Tell it which rows and columns\r#Freeze Panes (your co-workers will love you!)\r#Also check here: https://stackoverflow.com/questions/37677326/applying-style-to-all-sheets-of-a-workbook-using-openxlsx-package-in-r\rfreezePane(wb, \u0026quot;hr\u0026quot;, firstActiveRow = 2) #Freeze those panes. You know you want to. Tell it where to start.\r#Add filter\raddFilter(wb, \u0026quot;hr\u0026quot;, row = 1, cols = 1:4) #Add your filter as well. If you\u0026#39;re trying to impress, you might as well go all in :)\r#########################\r# hr_2 #\r#########################\raddWorksheet(wb, \u0026quot;hr_2\u0026quot;) #name the worksheet in Excel. This does not have to be the same as the df going into it.\rwriteData(wb, \u0026quot;hr_2\u0026quot;, hr_2) # write the `hr_2` data into your worksheet \u0026quot;hr_2\u0026quot;.\r#Create style\rstyle1 \u0026lt;- createStyle(fontColour = \u0026quot;#000080\u0026quot;, textDecoration = \u0026quot;Bold\u0026quot;) #Choose your custom font color (https://www.rgbtohex.net/) and make it bold. Call it style1\raddStyle(wb, style = style1, rows = 1, cols = 1:4, sheet = \u0026quot;hr_2\u0026quot;) #add this style to your worksheet. Tell it which rows and columns\r#Freeze Panes (your co-workers will love you!)\r#Also check here: https://stackoverflow.com/questions/37677326/applying-style-to-all-sheets-of-a-workbook-using-openxlsx-package-in-r\rfreezePane(wb, \u0026quot;hr_2\u0026quot;, firstActiveRow = 2) #Freeze those panes. You know you want to. Tell it where to start.\r#Add filter\raddFilter(wb, \u0026quot;hr_2\u0026quot;, row = 1, cols = 1:4) #Add your filter as well. If you\u0026#39;re trying to impress, you might as well go all in :)\r#########################\r# performance #\r#########################\raddWorksheet(wb, \u0026quot;performance\u0026quot;) #name the worksheet in Excel. This does not have to be the same as the df going into it.\rwriteData(wb, \u0026quot;performance\u0026quot;, performance) # write the `performance` data into your worksheet \u0026quot;performance\u0026quot;.\r#Create style\rstyle1 \u0026lt;- createStyle(fontColour = \u0026quot;#000080\u0026quot;, textDecoration = \u0026quot;Bold\u0026quot;) #Choose your custom font color (https://www.rgbtohex.net/) and make it bold. Call it style1\raddStyle(wb, style = style1, rows = 1, cols = 1:2, sheet = \u0026quot;performance\u0026quot;) #add this style to your worksheet. Tell it which rows and columns\r#Freeze Panes (your co-workers will love you!)\r#Also check here: https://stackoverflow.com/questions/37677326/applying-style-to-all-sheets-of-a-workbook-using-openxlsx-package-in-r\rfreezePane(wb, \u0026quot;performance\u0026quot;, firstActiveRow = 2) #Freeze those panes. You know you want to. Tell it where to start.\r#Add filter\raddFilter(wb, \u0026quot;performance\u0026quot;, row = 1, cols = 1:2) #Add your filter as well. If you\u0026#39;re trying to impress, you might as well go all in :)\r#########################\r# recruitment #\r#########################\raddWorksheet(wb, \u0026quot;recruitment\u0026quot;) #name the worksheet in Excel. This does not have to be the same as the df going into it.\rwriteData(wb, \u0026quot;recruitment\u0026quot;, recruitment) # write the `recruitment` data into your worksheet \u0026quot;recruitment\u0026quot;.\r#Create style\rstyle1 \u0026lt;- createStyle(fontColour = \u0026quot;#000080\u0026quot;, textDecoration = \u0026quot;Bold\u0026quot;) #Choose your custom font color (https://www.rgbtohex.net/) and make it bold. Call it style1\raddStyle(wb, style = style1, rows = 1, cols = 1:4, sheet = \u0026quot;recruitment\u0026quot;) #add this style to your worksheet. Tell it which rows and columns\r#Freeze Panes (your co-workers will love you!)\r#Also check here: https://stackoverflow.com/questions/37677326/applying-style-to-all-sheets-of-a-workbook-using-openxlsx-package-in-r\rfreezePane(wb, \u0026quot;recruitment\u0026quot;, firstActiveRow = 2) #Freeze those panes. You know you want to. Tell it where to start.\r#Add filter\raddFilter(wb, \u0026quot;recruitment\u0026quot;, row = 1, cols = 1:4) #Add your filter as well. If you\u0026#39;re trying to impress, you might as well go all in :)\r#########################\r# survey #\r#########################\raddWorksheet(wb, \u0026quot;survey\u0026quot;) #name the worksheet in Excel. This does not have to be the same as the df going into it.\rwriteData(wb, \u0026quot;survey\u0026quot;, survey) # write the `survey` data into your worksheet \u0026quot;survey\u0026quot;.\r#Create style\rstyle1 \u0026lt;- createStyle(fontColour = \u0026quot;#000080\u0026quot;, textDecoration = \u0026quot;Bold\u0026quot;) #Choose your custom font color (https://www.rgbtohex.net/) and make it bold. Call it style1\raddStyle(wb, style = style1, rows = 1, cols = 1:5, sheet = \u0026quot;survey\u0026quot;) #add this style to your worksheet. Tell it which rows and columns\r#Freeze Panes (your co-workers will love you!)\r#Also check here: https://stackoverflow.com/questions/37677326/applying-style-to-all-sheets-of-a-workbook-using-openxlsx-package-in-r\rfreezePane(wb, \u0026quot;survey\u0026quot;, firstActiveRow = 2) #Freeze those panes. You know you want to. Tell it where to start.\r#Add filter\raddFilter(wb, \u0026quot;survey\u0026quot;, row = 1, cols = 1:5) #Add your filter as well. If you\u0026#39;re trying to impress, you might as well go all in :)\r#########################\r# survey_2 #\r#########################\raddWorksheet(wb, \u0026quot;survey_2\u0026quot;) #name the worksheet in Excel. This does not have to be the same as the df going into it.\rwriteData(wb, \u0026quot;survey_2\u0026quot;, survey_2) # write the `survey_2` data into your worksheet \u0026quot;survey_2\u0026quot;.\r#Create style\rstyle1 \u0026lt;- createStyle(fontColour = \u0026quot;#000080\u0026quot;, textDecoration = \u0026quot;Bold\u0026quot;) #Choose your custom font color (https://www.rgbtohex.net/) and make it bold. Call it style1\raddStyle(wb, style = style1, rows = 1, cols = 1:3, sheet = \u0026quot;survey_2\u0026quot;) #add this style to your worksheet. Tell it which rows and columns\r#Freeze Panes (your co-workers will love you!)\r#Also check here: https://stackoverflow.com/questions/37677326/applying-style-to-all-sheets-of-a-workbook-using-openxlsx-package-in-r\rfreezePane(wb, \u0026quot;survey_2\u0026quot;, firstActiveRow = 2) #Freeze those panes. You know you want to. Tell it where to start.\r#Add filter\raddFilter(wb, \u0026quot;survey_2\u0026quot;, row = 1, cols = 1:3) #Add your filter as well. If you\u0026#39;re trying to impress, you might as well go all in :)\r#Now we\u0026#39;ll do a fancy save by customizing the file name using paste0 and system time. We\u0026#39;ll also assume this was for the previous month. You\u0026#39;ll also need to make this path the one you want on your computer. saveWorkbook(wb, paste0(\u0026quot;00_Data/Additional Data for Attrition Data Set_\u0026quot;, format(Sys.Date(), \u0026quot;%d_%m_%Y\u0026quot;) , \u0026quot;.xlsx\u0026quot;), overwrite = TRUE)\r\r","date":1599609600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599696124,"objectID":"5e3621706959bd8c97484781c757d07e","permalink":"http://robstilson.rbind.io/post/working-with-data-part-1-getting-data-from-github/","publishdate":"2020-09-09T00:00:00Z","relpermalink":"/post/working-with-data-part-1-getting-data-from-github/","section":"post","summary":"Summary\rGet Packages\rLoad packages\rBring in the data\rStructure of GitHub pull\rTake a look at the data\rSkimr::skim\rjanitor::tabyl\r\r\rBring in more data\rExporting the data to Excel with openxlsx\r\rWrap up\rJust the code\r\r\rSummary\rIn this post I will introduce you to getting data from existing GitHub sites.","tags":[],"title":"Working with Data-Part 1-Getting data from GitHub","type":"post"},{"authors":[],"categories":["EDA"],"content":"\r\rSummary\rGet Packages\rLoad packages\rBring in the data\rLoad data from Excel\rHead\rTail\rHeadTail\rDimensions of the data\rStructure of the data\rGlimpse the data\rSkim the data\rDescribe the data\rSave describe as a DF\rRound describe and save as a DF\rWrite to Excel\rAdd an ID column\r\rCharacter Data\rNumeric Data\rWrap up\rJust the code\r\r\rSummary\rIn this post I will introduce you to HR data from the IBM Watson Analytics Lab. The modeldata package contains this as the attrition data set. This data set is also available from Kaggle. Here is the description from the Details portion of the documentation.\nThese data are from the IBM Watson Analytics Lab. The website describes the data with “Uncover the factors that lead to employee attrition and explore important questions such as ‘show me a breakdown of distance from home by job role and attrition’ or ‘compare average monthly income by education and attrition’. This is a fictional data set created by IBM data scientists.”. There are 1470 rows.\nGet Packages\rThis code chunk will load the necessary packages for the analysis. I’ve been using pacman for years and I encourage you to check it out as well. The tryCatch portion of the code will check your machine to see if pacman has already been installed. If it finds pacman, it will download it and if not it will move on. The pacman::p_load function is a wrapper for library and require and checks to see if the listed packages are installed. If they are not, it will attempt to install them from CRAN and/or any other repository in the pacman repository list (See Description under ?p_load for additional information).\nAs a best practice, I also like to give a brief snippet of what each of the packages I’m loading will do in the analysis. This is helpful for anybody else who I send my code to, but also for myself if I need to come back to the code 6 months or more down the line.\n\rLoad packages\rHere I’m explicitly calling the packages via library. Notice that tidylog is at the very end. This is necessary to load after dplyr so that all of the functions from tidylog work correctly. This is used in conjunction with the conflicted package. You can read more about it here.\nconflict_prefer(\u0026quot;filter\u0026quot;, \u0026quot;dplyr\u0026quot;)\r## [conflicted] Removing existing preference\r## [conflicted] Will prefer dplyr::filter over any other package\r\rBring in the data\rFor this demonstration we’ll use attrition directly from the modeldata package. We’ll rename it Data so we don’t overwrite anything.\nlibrary(modeldata)\rdata(attrition)\rData \u0026lt;- attrition\r\rLoad data from Excel\rIf you downloaded the data set from Kaggle as an Excel file, you would used something like below. The easiest way to do this is probably to use File -\u0026gt; Import Data Set -\u0026gt; From Excel from the RStudio drop down menu.\nlibrary(readxl)\rData \u0026lt;- read_excel(\u0026quot;D:/Folder where the file is located/WA_Fn-UseC_-HR-Employee-Attrition.xlsx\u0026quot;)\r\rHead\rOk, now that we’ve got our data into R, let’s start looking around.\nFirst, we’ll use the head function to see the first few rows.\nhead(Data)\r## Age Attrition BusinessTravel DailyRate Department\r## 1 41 Yes Travel_Rarely 1102 Sales\r## 2 49 No Travel_Frequently 279 Research_Development\r## 4 37 Yes Travel_Rarely 1373 Research_Development\r## 5 33 No Travel_Frequently 1392 Research_Development\r## 7 27 No Travel_Rarely 591 Research_Development\r## 8 32 No Travel_Frequently 1005 Research_Development\r## DistanceFromHome Education EducationField EnvironmentSatisfaction Gender\r## 1 1 College Life_Sciences Medium Female\r## 2 8 Below_College Life_Sciences High Male\r## 4 2 College Other Very_High Male\r## 5 3 Master Life_Sciences Very_High Female\r## 7 2 Below_College Medical Low Male\r## 8 2 College Life_Sciences Very_High Male\r## HourlyRate JobInvolvement JobLevel JobRole JobSatisfaction\r## 1 94 High 2 Sales_Executive Very_High\r## 2 61 Medium 2 Research_Scientist Medium\r## 4 92 Medium 1 Laboratory_Technician High\r## 5 56 High 1 Research_Scientist High\r## 7 40 High 1 Laboratory_Technician Medium\r## 8 79 High 1 Laboratory_Technician Very_High\r## MaritalStatus MonthlyIncome MonthlyRate NumCompaniesWorked OverTime\r## 1 Single 5993 19479 8 Yes\r## 2 Married 5130 24907 1 No\r## 4 Single 2090 2396 6 Yes\r## 5 Married 2909 23159 1 Yes\r## 7 Married 3468 16632 9 No\r## 8 Single 3068 11864 0 No\r## PercentSalaryHike PerformanceRating RelationshipSatisfaction StockOptionLevel\r## 1 11 Excellent Low 0\r## 2 23 Outstanding Very_High 1\r## 4 15 Excellent Medium 0\r## 5 11 Excellent High 0\r## 7 12 Excellent Very_High 1\r## 8 13 Excellent High 0\r## TotalWorkingYears TrainingTimesLastYear WorkLifeBalance YearsAtCompany\r## 1 8 0 Bad 6\r## 2 10 3 Better 10\r## 4 7 3 Better 0\r## 5 8 3 Better 8\r## 7 6 3 Better 2\r## 8 8 2 Good 7\r## YearsInCurrentRole YearsSinceLastPromotion YearsWithCurrManager\r## 1 4 0 5\r## 2 7 1 7\r## 4 0 0 0\r## 5 7 3 0\r## 7 2 2 2\r## 8 7 3 6\rIf we wanted to see the first 10 rows instead of the first 6 rows we can change it up with n.\nhead(Data, n = 10)\r## Age Attrition BusinessTravel DailyRate Department\r## 1 41 Yes Travel_Rarely 1102 Sales\r## 2 49 No Travel_Frequently 279 Research_Development\r## 4 37 Yes Travel_Rarely 1373 Research_Development\r## 5 33 No Travel_Frequently 1392 Research_Development\r## 7 27 No Travel_Rarely 591 Research_Development\r## 8 32 No Travel_Frequently 1005 Research_Development\r## 10 59 No Travel_Rarely 1324 Research_Development\r## 11 30 No Travel_Rarely 1358 Research_Development\r## 12 38 No Travel_Frequently 216 Research_Development\r## 13 36 No Travel_Rarely 1299 Research_Development\r## DistanceFromHome Education EducationField EnvironmentSatisfaction Gender\r## 1 1 College Life_Sciences Medium Female\r## 2 8 Below_College Life_Sciences High Male\r## 4 2 College Other Very_High Male\r## 5 3 Master Life_Sciences Very_High Female\r## 7 2 Below_College Medical Low Male\r## 8 2 College Life_Sciences Very_High Male\r## 10 3 Bachelor Medical High Female\r## 11 24 Below_College Life_Sciences Very_High Male\r## 12 23 Bachelor Life_Sciences Very_High Male\r## 13 27 Bachelor Medical High Male\r## HourlyRate JobInvolvement JobLevel JobRole JobSatisfaction\r## 1 94 High 2 Sales_Executive Very_High\r## 2 61 Medium 2 Research_Scientist Medium\r## 4 92 Medium 1 Laboratory_Technician High\r## 5 56 High 1 Research_Scientist High\r## 7 40 High 1 Laboratory_Technician Medium\r## 8 79 High 1 Laboratory_Technician Very_High\r## 10 81 Very_High 1 Laboratory_Technician Low\r## 11 67 High 1 Laboratory_Technician High\r## 12 44 Medium 3 Manufacturing_Director High\r## 13 94 High 2 Healthcare_Representative High\r## MaritalStatus MonthlyIncome MonthlyRate NumCompaniesWorked OverTime\r## 1 Single 5993 19479 8 Yes\r## 2 Married 5130 24907 1 No\r## 4 Single 2090 2396 6 Yes\r## 5 Married 2909 23159 1 Yes\r## 7 Married 3468 16632 9 No\r## 8 Single 3068 11864 0 No\r## 10 Married 2670 9964 4 Yes\r## 11 Divorced 2693 13335 1 No\r## 12 Single 9526 8787 0 No\r## 13 Married 5237 16577 6 No\r## PercentSalaryHike PerformanceRating RelationshipSatisfaction\r## 1 11 Excellent Low\r## 2 23 Outstanding Very_High\r## 4 15 Excellent Medium\r## 5 11 Excellent High\r## 7 12 Excellent Very_High\r## 8 13 Excellent High\r## 10 20 Outstanding Low\r## 11 22 Outstanding Medium\r## 12 21 Outstanding Medium\r## 13 13 Excellent Medium\r## StockOptionLevel TotalWorkingYears TrainingTimesLastYear WorkLifeBalance\r## 1 0 8 0 Bad\r## 2 1 10 3 Better\r## 4 0 7 3 Better\r## 5 0 8 3 Better\r## 7 1 6 3 Better\r## 8 0 8 2 Good\r## 10 3 12 3 Good\r## 11 1 1 2 Better\r## 12 0 10 2 Better\r## 13 2 17 3 Good\r## YearsAtCompany YearsInCurrentRole YearsSinceLastPromotion\r## 1 6 4 0\r## 2 10 7 1\r## 4 0 0 0\r## 5 8 7 3\r## 7 2 2 2\r## 8 7 7 3\r## 10 1 0 0\r## 11 1 0 0\r## 12 9 7 1\r## 13 7 7 7\r## YearsWithCurrManager\r## 1 5\r## 2 7\r## 4 0\r## 5 0\r## 7 2\r## 8 6\r## 10 0\r## 11 0\r## 12 8\r## 13 7\r\rTail\rWe can also take a look at the bottom of the data with tail.\ntail(Data)\r## Age Attrition BusinessTravel DailyRate Department\r## 2060 26 No Travel_Rarely 1167 Sales\r## 2061 36 No Travel_Frequently 884 Research_Development\r## 2062 39 No Travel_Rarely 613 Research_Development\r## 2064 27 No Travel_Rarely 155 Research_Development\r## 2065 49 No Travel_Frequently 1023 Sales\r## 2068 34 No Travel_Rarely 628 Research_Development\r## DistanceFromHome Education EducationField EnvironmentSatisfaction\r## 2060 5 Bachelor Other Very_High\r## 2061 23 College Medical High\r## 2062 6 Below_College Medical Very_High\r## 2064 4 Bachelor Life_Sciences Medium\r## 2065 2 Bachelor Medical Very_High\r## 2068 8 Bachelor Medical Medium\r## Gender HourlyRate JobInvolvement JobLevel JobRole\r## 2060 Female 30 Medium 1 Sales_Representative\r## 2061 Male 41 Very_High 2 Laboratory_Technician\r## 2062 Male 42 Medium 3 Healthcare_Representative\r## 2064 Male 87 Very_High 2 Manufacturing_Director\r## 2065 Male 63 Medium 2 Sales_Executive\r## 2068 Male 82 Very_High 2 Laboratory_Technician\r## JobSatisfaction MaritalStatus MonthlyIncome MonthlyRate NumCompaniesWorked\r## 2060 High Single 2966 21378 0\r## 2061 Very_High Married 2571 12290 4\r## 2062 Low Married 9991 21457 4\r## 2064 Medium Married 6142 5174 1\r## 2065 Medium Married 5390 13243 2\r## 2068 High Married 4404 10228 2\r## OverTime PercentSalaryHike PerformanceRating RelationshipSatisfaction\r## 2060 No 18 Excellent Very_High\r## 2061 No 17 Excellent High\r## 2062 No 15 Excellent Low\r## 2064 Yes 20 Outstanding Medium\r## 2065 No 14 Excellent Very_High\r## 2068 No 12 Excellent Low\r## StockOptionLevel TotalWorkingYears TrainingTimesLastYear WorkLifeBalance\r## 2060 0 5 2 Better\r## 2061 1 17 3 Better\r## 2062 1 9 5 Better\r## 2064 1 6 0 Better\r## 2065 0 17 3 Good\r## 2068 0 6 3 Best\r## YearsAtCompany YearsInCurrentRole YearsSinceLastPromotion\r## 2060 4 2 0\r## 2061 5 2 0\r## 2062 7 7 1\r## 2064 6 2 0\r## 2065 9 6 0\r## 2068 4 3 1\r## YearsWithCurrManager\r## 2060 0\r## 2061 3\r## 2062 7\r## 2064 3\r## 2065 8\r## 2068 2\r\rHeadTail\rAnd, if we would like to see both at once, we can use the headTail function from the psych package.\nheadTail(Data)\r## Age Attrition BusinessTravel DailyRate Department\r## 1 41 Yes Travel_Rarely 1102 Sales\r## 2 49 No Travel_Frequently 279 Research_Development\r## 4 37 Yes Travel_Rarely 1373 Research_Development\r## 5 33 No Travel_Frequently 1392 Research_Development\r## ... ... \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ... \u0026lt;NA\u0026gt;\r## 2062 39 No Travel_Rarely 613 Research_Development\r## 2064 27 No Travel_Rarely 155 Research_Development\r## 2065 49 No Travel_Frequently 1023 Sales\r## 2068 34 No Travel_Rarely 628 Research_Development\r## DistanceFromHome Education EducationField EnvironmentSatisfaction\r## 1 1 College Life_Sciences Medium\r## 2 8 Below_College Life_Sciences High\r## 4 2 College Other Very_High\r## 5 3 Master Life_Sciences Very_High\r## ... ... \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt;\r## 2062 6 Below_College Medical Very_High\r## 2064 4 Bachelor Life_Sciences Medium\r## 2065 2 Bachelor Medical Very_High\r## 2068 8 Bachelor Medical Medium\r## Gender HourlyRate JobInvolvement JobLevel JobRole\r## 1 Female 94 High 2 Sales_Executive\r## 2 Male 61 Medium 2 Research_Scientist\r## 4 Male 92 Medium 1 Laboratory_Technician\r## 5 Female 56 High 1 Research_Scientist\r## ... \u0026lt;NA\u0026gt; ... \u0026lt;NA\u0026gt; ... \u0026lt;NA\u0026gt;\r## 2062 Male 42 Medium 3 Healthcare_Representative\r## 2064 Male 87 Very_High 2 Manufacturing_Director\r## 2065 Male 63 Medium 2 Sales_Executive\r## 2068 Male 82 Very_High 2 Laboratory_Technician\r## JobSatisfaction MaritalStatus MonthlyIncome MonthlyRate NumCompaniesWorked\r## 1 Very_High Single 5993 19479 8\r## 2 Medium Married 5130 24907 1\r## 4 High Single 2090 2396 6\r## 5 High Married 2909 23159 1\r## ... \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ... ... ...\r## 2062 Low Married 9991 21457 4\r## 2064 Medium Married 6142 5174 1\r## 2065 Medium Married 5390 13243 2\r## 2068 High Married 4404 10228 2\r## OverTime PercentSalaryHike PerformanceRating RelationshipSatisfaction\r## 1 Yes 11 Excellent Low\r## 2 No 23 Outstanding Very_High\r## 4 Yes 15 Excellent Medium\r## 5 Yes 11 Excellent High\r## ... \u0026lt;NA\u0026gt; ... \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt;\r## 2062 No 15 Excellent Low\r## 2064 Yes 20 Outstanding Medium\r## 2065 No 14 Excellent Very_High\r## 2068 No 12 Excellent Low\r## StockOptionLevel TotalWorkingYears TrainingTimesLastYear WorkLifeBalance\r## 1 0 8 0 Bad\r## 2 1 10 3 Better\r## 4 0 7 3 Better\r## 5 0 8 3 Better\r## ... ... ... ... \u0026lt;NA\u0026gt;\r## 2062 1 9 5 Better\r## 2064 1 6 0 Better\r## 2065 0 17 3 Good\r## 2068 0 6 3 Best\r## YearsAtCompany YearsInCurrentRole YearsSinceLastPromotion\r## 1 6 4 0\r## 2 10 7 1\r## 4 0 0 0\r## 5 8 7 3\r## ... ... ... ...\r## 2062 7 7 1\r## 2064 6 2 0\r## 2065 9 6 0\r## 2068 4 3 1\r## YearsWithCurrManager\r## 1 5\r## 2 7\r## 4 0\r## 5 0\r## ... ...\r## 2062 7\r## 2064 3\r## 2065 8\r## 2068 2\rIf we wanted to use the headTail function without loading the psych library or if we want to make it explicit that we are using that function from that package, we could have done the following as well.\npsych::headTail(Data)\r\rDimensions of the data\rWe can check out the dimensions of the data to get the number of rows and columns.\ndim(Data)\r## [1] 1470 31\r\rStructure of the data\rWe can get the structure of the data.\nstr(Data)\r## \u0026#39;data.frame\u0026#39;: 1470 obs. of 31 variables:\r## $ Age : int 41 49 37 33 27 32 59 30 38 36 ...\r## $ Attrition : Factor w/ 2 levels \u0026quot;No\u0026quot;,\u0026quot;Yes\u0026quot;: 2 1 2 1 1 1 1 1 1 1 ...\r## $ BusinessTravel : Factor w/ 3 levels \u0026quot;Non-Travel\u0026quot;,\u0026quot;Travel_Frequently\u0026quot;,..: 3 2 3 2 3 2 3 3 2 3 ...\r## $ DailyRate : int 1102 279 1373 1392 591 1005 1324 1358 216 1299 ...\r## $ Department : Factor w/ 3 levels \u0026quot;Human_Resources\u0026quot;,..: 3 2 2 2 2 2 2 2 2 2 ...\r## $ DistanceFromHome : int 1 8 2 3 2 2 3 24 23 27 ...\r## $ Education : Ord.factor w/ 5 levels \u0026quot;Below_College\u0026quot;\u0026lt;..: 2 1 2 4 1 2 3 1 3 3 ...\r## $ EducationField : Factor w/ 6 levels \u0026quot;Human_Resources\u0026quot;,..: 2 2 5 2 4 2 4 2 2 4 ...\r## $ EnvironmentSatisfaction : Ord.factor w/ 4 levels \u0026quot;Low\u0026quot;\u0026lt;\u0026quot;Medium\u0026quot;\u0026lt;..: 2 3 4 4 1 4 3 4 4 3 ...\r## $ Gender : Factor w/ 2 levels \u0026quot;Female\u0026quot;,\u0026quot;Male\u0026quot;: 1 2 2 1 2 2 1 2 2 2 ...\r## $ HourlyRate : int 94 61 92 56 40 79 81 67 44 94 ...\r## $ JobInvolvement : Ord.factor w/ 4 levels \u0026quot;Low\u0026quot;\u0026lt;\u0026quot;Medium\u0026quot;\u0026lt;..: 3 2 2 3 3 3 4 3 2 3 ...\r## $ JobLevel : int 2 2 1 1 1 1 1 1 3 2 ...\r## $ JobRole : Factor w/ 9 levels \u0026quot;Healthcare_Representative\u0026quot;,..: 8 7 3 7 3 3 3 3 5 1 ...\r## $ JobSatisfaction : Ord.factor w/ 4 levels \u0026quot;Low\u0026quot;\u0026lt;\u0026quot;Medium\u0026quot;\u0026lt;..: 4 2 3 3 2 4 1 3 3 3 ...\r## $ MaritalStatus : Factor w/ 3 levels \u0026quot;Divorced\u0026quot;,\u0026quot;Married\u0026quot;,..: 3 2 3 2 2 3 2 1 3 2 ...\r## $ MonthlyIncome : int 5993 5130 2090 2909 3468 3068 2670 2693 9526 5237 ...\r## $ MonthlyRate : int 19479 24907 2396 23159 16632 11864 9964 13335 8787 16577 ...\r## $ NumCompaniesWorked : int 8 1 6 1 9 0 4 1 0 6 ...\r## $ OverTime : Factor w/ 2 levels \u0026quot;No\u0026quot;,\u0026quot;Yes\u0026quot;: 2 1 2 2 1 1 2 1 1 1 ...\r## $ PercentSalaryHike : int 11 23 15 11 12 13 20 22 21 13 ...\r## $ PerformanceRating : Ord.factor w/ 4 levels \u0026quot;Low\u0026quot;\u0026lt;\u0026quot;Good\u0026quot;\u0026lt;\u0026quot;Excellent\u0026quot;\u0026lt;..: 3 4 3 3 3 3 4 4 4 3 ...\r## $ RelationshipSatisfaction: Ord.factor w/ 4 levels \u0026quot;Low\u0026quot;\u0026lt;\u0026quot;Medium\u0026quot;\u0026lt;..: 1 4 2 3 4 3 1 2 2 2 ...\r## $ StockOptionLevel : int 0 1 0 0 1 0 3 1 0 2 ...\r## $ TotalWorkingYears : int 8 10 7 8 6 8 12 1 10 17 ...\r## $ TrainingTimesLastYear : int 0 3 3 3 3 2 3 2 2 3 ...\r## $ WorkLifeBalance : Ord.factor w/ 4 levels \u0026quot;Bad\u0026quot;\u0026lt;\u0026quot;Good\u0026quot;\u0026lt;\u0026quot;Better\u0026quot;\u0026lt;..: 1 3 3 3 3 2 2 3 3 2 ...\r## $ YearsAtCompany : int 6 10 0 8 2 7 1 1 9 7 ...\r## $ YearsInCurrentRole : int 4 7 0 7 2 7 0 0 7 7 ...\r## $ YearsSinceLastPromotion : int 0 1 0 3 2 3 0 0 1 7 ...\r## $ YearsWithCurrManager : int 5 7 0 0 2 6 0 0 8 7 ...\r\rGlimpse the data\rA similar way to get this information is the glimpse function from dplyr. A benefit of glimpse is there doesn’t seem to be a limit on the number of columns it can display where str seems to have a limit of the first 99.\nglimpse(Data)\r## Rows: 1,470\r## Columns: 31\r## $ Age \u0026lt;int\u0026gt; 41, 49, 37, 33, 27, 32, 59, 30, 38, 36, 35...\r## $ Attrition \u0026lt;fct\u0026gt; Yes, No, Yes, No, No, No, No, No, No, No, ...\r## $ BusinessTravel \u0026lt;fct\u0026gt; Travel_Rarely, Travel_Frequently, Travel_R...\r## $ DailyRate \u0026lt;int\u0026gt; 1102, 279, 1373, 1392, 591, 1005, 1324, 13...\r## $ Department \u0026lt;fct\u0026gt; Sales, Research_Development, Research_Deve...\r## $ DistanceFromHome \u0026lt;int\u0026gt; 1, 8, 2, 3, 2, 2, 3, 24, 23, 27, 16, 15, 2...\r## $ Education \u0026lt;ord\u0026gt; College, Below_College, College, Master, B...\r## $ EducationField \u0026lt;fct\u0026gt; Life_Sciences, Life_Sciences, Other, Life_...\r## $ EnvironmentSatisfaction \u0026lt;ord\u0026gt; Medium, High, Very_High, Very_High, Low, V...\r## $ Gender \u0026lt;fct\u0026gt; Female, Male, Male, Female, Male, Male, Fe...\r## $ HourlyRate \u0026lt;int\u0026gt; 94, 61, 92, 56, 40, 79, 81, 67, 44, 94, 84...\r## $ JobInvolvement \u0026lt;ord\u0026gt; High, Medium, Medium, High, High, High, Ve...\r## $ JobLevel \u0026lt;int\u0026gt; 2, 2, 1, 1, 1, 1, 1, 1, 3, 2, 1, 2, 1, 1, ...\r## $ JobRole \u0026lt;fct\u0026gt; Sales_Executive, Research_Scientist, Labor...\r## $ JobSatisfaction \u0026lt;ord\u0026gt; Very_High, Medium, High, High, Medium, Ver...\r## $ MaritalStatus \u0026lt;fct\u0026gt; Single, Married, Single, Married, Married,...\r## $ MonthlyIncome \u0026lt;int\u0026gt; 5993, 5130, 2090, 2909, 3468, 3068, 2670, ...\r## $ MonthlyRate \u0026lt;int\u0026gt; 19479, 24907, 2396, 23159, 16632, 11864, 9...\r## $ NumCompaniesWorked \u0026lt;int\u0026gt; 8, 1, 6, 1, 9, 0, 4, 1, 0, 6, 0, 0, 1, 0, ...\r## $ OverTime \u0026lt;fct\u0026gt; Yes, No, Yes, Yes, No, No, Yes, No, No, No...\r## $ PercentSalaryHike \u0026lt;int\u0026gt; 11, 23, 15, 11, 12, 13, 20, 22, 21, 13, 13...\r## $ PerformanceRating \u0026lt;ord\u0026gt; Excellent, Outstanding, Excellent, Excelle...\r## $ RelationshipSatisfaction \u0026lt;ord\u0026gt; Low, Very_High, Medium, High, Very_High, H...\r## $ StockOptionLevel \u0026lt;int\u0026gt; 0, 1, 0, 0, 1, 0, 3, 1, 0, 2, 1, 0, 1, 1, ...\r## $ TotalWorkingYears \u0026lt;int\u0026gt; 8, 10, 7, 8, 6, 8, 12, 1, 10, 17, 6, 10, 5...\r## $ TrainingTimesLastYear \u0026lt;int\u0026gt; 0, 3, 3, 3, 3, 2, 3, 2, 2, 3, 5, 3, 1, 2, ...\r## $ WorkLifeBalance \u0026lt;ord\u0026gt; Bad, Better, Better, Better, Better, Good,...\r## $ YearsAtCompany \u0026lt;int\u0026gt; 6, 10, 0, 8, 2, 7, 1, 1, 9, 7, 5, 9, 5, 2,...\r## $ YearsInCurrentRole \u0026lt;int\u0026gt; 4, 7, 0, 7, 2, 7, 0, 0, 7, 7, 4, 5, 2, 2, ...\r## $ YearsSinceLastPromotion \u0026lt;int\u0026gt; 0, 1, 0, 3, 2, 3, 0, 0, 1, 7, 0, 0, 4, 1, ...\r## $ YearsWithCurrManager \u0026lt;int\u0026gt; 5, 7, 0, 0, 2, 6, 0, 0, 8, 7, 3, 8, 3, 2, ...\r\rSkim the data\rThe skimr package is useful to get some additional information on the data frame (df) with the skim function. In addition to number of columns and rows, it gives you the following:\n\rcolumn type with frequency\rany grouping variables\reach column with number missing\rif the column is ordered\rnumber of unique occurrences in each column\rtop counts in each column\rmean\rstandard deviation\rquantiles\ra mini histogram of the data\r\rskim(Data)\r\rTable 1: Data summary\r\rName\rData\r\rNumber of rows\r1470\r\rNumber of columns\r31\r\r_______________________\r\r\rColumn type frequency:\r\r\rfactor\r15\r\rnumeric\r16\r\r________________________\r\r\rGroup variables\rNone\r\r\r\rVariable type: factor\n\r\rskim_variable\rn_missing\rcomplete_rate\rordered\rn_unique\rtop_counts\r\r\r\rAttrition\r0\r1\rFALSE\r2\rNo: 1233, Yes: 237\r\rBusinessTravel\r0\r1\rFALSE\r3\rTra: 1043, Tra: 277, Non: 150\r\rDepartment\r0\r1\rFALSE\r3\rRes: 961, Sal: 446, Hum: 63\r\rEducation\r0\r1\rTRUE\r5\rBac: 572, Mas: 398, Col: 282, Bel: 170\r\rEducationField\r0\r1\rFALSE\r6\rLif: 606, Med: 464, Mar: 159, Tec: 132\r\rEnvironmentSatisfaction\r0\r1\rTRUE\r4\rHig: 453, Ver: 446, Med: 287, Low: 284\r\rGender\r0\r1\rFALSE\r2\rMal: 882, Fem: 588\r\rJobInvolvement\r0\r1\rTRUE\r4\rHig: 868, Med: 375, Ver: 144, Low: 83\r\rJobRole\r0\r1\rFALSE\r9\rSal: 326, Res: 292, Lab: 259, Man: 145\r\rJobSatisfaction\r0\r1\rTRUE\r4\rVer: 459, Hig: 442, Low: 289, Med: 280\r\rMaritalStatus\r0\r1\rFALSE\r3\rMar: 673, Sin: 470, Div: 327\r\rOverTime\r0\r1\rFALSE\r2\rNo: 1054, Yes: 416\r\rPerformanceRating\r0\r1\rTRUE\r2\rExc: 1244, Out: 226, Low: 0, Goo: 0\r\rRelationshipSatisfaction\r0\r1\rTRUE\r4\rHig: 459, Ver: 432, Med: 303, Low: 276\r\rWorkLifeBalance\r0\r1\rTRUE\r4\rBet: 893, Goo: 344, Bes: 153, Bad: 80\r\r\r\rVariable type: numeric\n\r\rskim_variable\rn_missing\rcomplete_rate\rmean\rsd\rp0\rp25\rp50\rp75\rp100\rhist\r\r\r\rAge\r0\r1\r36.92\r9.14\r18\r30\r36.0\r43.00\r60\r▂▇▇▃▂\r\rDailyRate\r0\r1\r802.49\r403.51\r102\r465\r802.0\r1157.00\r1499\r▇▇▇▇▇\r\rDistanceFromHome\r0\r1\r9.19\r8.11\r1\r2\r7.0\r14.00\r29\r▇▅▂▂▂\r\rHourlyRate\r0\r1\r65.89\r20.33\r30\r48\r66.0\r83.75\r100\r▇▇▇▇▇\r\rJobLevel\r0\r1\r2.06\r1.11\r1\r1\r2.0\r3.00\r5\r▇▇▃▂▁\r\rMonthlyIncome\r0\r1\r6502.93\r4707.96\r1009\r2911\r4919.0\r8379.00\r19999\r▇▅▂▁▂\r\rMonthlyRate\r0\r1\r14313.10\r7117.79\r2094\r8047\r14235.5\r20461.50\r26999\r▇▇▇▇▇\r\rNumCompaniesWorked\r0\r1\r2.69\r2.50\r0\r1\r2.0\r4.00\r9\r▇▃▂▂▁\r\rPercentSalaryHike\r0\r1\r15.21\r3.66\r11\r12\r14.0\r18.00\r25\r▇▅▃▂▁\r\rStockOptionLevel\r0\r1\r0.79\r0.85\r0\r0\r1.0\r1.00\r3\r▇▇▁▂▁\r\rTotalWorkingYears\r0\r1\r11.28\r7.78\r0\r6\r10.0\r15.00\r40\r▇▇▂▁▁\r\rTrainingTimesLastYear\r0\r1\r2.80\r1.29\r0\r2\r3.0\r3.00\r6\r▂▇▇▂▃\r\rYearsAtCompany\r0\r1\r7.01\r6.13\r0\r3\r5.0\r9.00\r40\r▇▂▁▁▁\r\rYearsInCurrentRole\r0\r1\r4.23\r3.62\r0\r2\r3.0\r7.00\r18\r▇▃▂▁▁\r\rYearsSinceLastPromotion\r0\r1\r2.19\r3.22\r0\r0\r1.0\r3.00\r15\r▇▁▁▁▁\r\rYearsWithCurrManager\r0\r1\r4.12\r3.57\r0\r2\r3.0\r7.00\r17\r▇▂▅▁▁\r\r\r\r\rDescribe the data\rWe can lean on the psych package again with the describe function to get some information about the df.\ndescribe(Data)\r## vars n mean sd median trimmed mad\r## Age 1 1470 36.92 9.14 36.0 36.47 8.90\r## Attrition* 2 1470 1.16 0.37 1.0 1.08 0.00\r## BusinessTravel* 3 1470 2.61 0.67 3.0 2.76 0.00\r## DailyRate 4 1470 802.49 403.51 802.0 803.83 510.01\r## Department* 5 1470 2.26 0.53 2.0 2.25 0.00\r## DistanceFromHome 6 1470 9.19 8.11 7.0 8.08 7.41\r## Education* 7 1470 2.91 1.02 3.0 2.98 1.48\r## EducationField* 8 1470 3.25 1.33 3.0 3.10 1.48\r## EnvironmentSatisfaction* 9 1470 2.72 1.09 3.0 2.78 1.48\r## Gender* 10 1470 1.60 0.49 2.0 1.62 0.00\r## HourlyRate 11 1470 65.89 20.33 66.0 66.02 26.69\r## JobInvolvement* 12 1470 2.73 0.71 3.0 2.74 0.00\r## JobLevel 13 1470 2.06 1.11 2.0 1.90 1.48\r## JobRole* 14 1470 5.46 2.46 6.0 5.61 2.97\r## JobSatisfaction* 15 1470 2.73 1.10 3.0 2.79 1.48\r## MaritalStatus* 16 1470 2.10 0.73 2.0 2.12 1.48\r## MonthlyIncome 17 1470 6502.93 4707.96 4919.0 5667.24 3260.24\r## MonthlyRate 18 1470 14313.10 7117.79 14235.5 14286.48 9201.76\r## NumCompaniesWorked 19 1470 2.69 2.50 2.0 2.36 1.48\r## OverTime* 20 1470 1.28 0.45 1.0 1.23 0.00\r## PercentSalaryHike 21 1470 15.21 3.66 14.0 14.80 2.97\r## PerformanceRating* 22 1470 3.15 0.36 3.0 3.07 0.00\r## RelationshipSatisfaction* 23 1470 2.71 1.08 3.0 2.77 1.48\r## StockOptionLevel 24 1470 0.79 0.85 1.0 0.67 1.48\r## TotalWorkingYears 25 1470 11.28 7.78 10.0 10.37 5.93\r## TrainingTimesLastYear 26 1470 2.80 1.29 3.0 2.72 1.48\r## WorkLifeBalance* 27 1470 2.76 0.71 3.0 2.77 0.00\r## YearsAtCompany 28 1470 7.01 6.13 5.0 5.99 4.45\r## YearsInCurrentRole 29 1470 4.23 3.62 3.0 3.85 4.45\r## YearsSinceLastPromotion 30 1470 2.19 3.22 1.0 1.48 1.48\r## YearsWithCurrManager 31 1470 4.12 3.57 3.0 3.77 4.45\r## min max range skew kurtosis se\r## Age 18 60 42 0.41 -0.41 0.24\r## Attrition* 1 2 1 1.84 1.39 0.01\r## BusinessTravel* 1 3 2 -1.44 0.69 0.02\r## DailyRate 102 1499 1397 0.00 -1.21 10.52\r## Department* 1 3 2 0.17 -0.40 0.01\r## DistanceFromHome 1 29 28 0.96 -0.23 0.21\r## Education* 1 5 4 -0.29 -0.56 0.03\r## EducationField* 1 6 5 0.55 -0.69 0.03\r## EnvironmentSatisfaction* 1 4 3 -0.32 -1.20 0.03\r## Gender* 1 2 1 -0.41 -1.83 0.01\r## HourlyRate 30 100 70 -0.03 -1.20 0.53\r## JobInvolvement* 1 4 3 -0.50 0.26 0.02\r## JobLevel 1 5 4 1.02 0.39 0.03\r## JobRole* 1 9 8 -0.36 -1.20 0.06\r## JobSatisfaction* 1 4 3 -0.33 -1.22 0.03\r## MaritalStatus* 1 3 2 -0.15 -1.12 0.02\r## MonthlyIncome 1009 19999 18990 1.37 0.99 122.79\r## MonthlyRate 2094 26999 24905 0.02 -1.22 185.65\r## NumCompaniesWorked 0 9 9 1.02 0.00 0.07\r## OverTime* 1 2 1 0.96 -1.07 0.01\r## PercentSalaryHike 11 25 14 0.82 -0.31 0.10\r## PerformanceRating* 3 4 1 1.92 1.68 0.01\r## RelationshipSatisfaction* 1 4 3 -0.30 -1.19 0.03\r## StockOptionLevel 0 3 3 0.97 0.35 0.02\r## TotalWorkingYears 0 40 40 1.11 0.91 0.20\r## TrainingTimesLastYear 0 6 6 0.55 0.48 0.03\r## WorkLifeBalance* 1 4 3 -0.55 0.41 0.02\r## YearsAtCompany 0 40 40 1.76 3.91 0.16\r## YearsInCurrentRole 0 18 18 0.92 0.47 0.09\r## YearsSinceLastPromotion 0 15 15 1.98 3.59 0.08\r## YearsWithCurrManager 0 17 17 0.83 0.16 0.09\r\rSave describe as a DF\rIf you want, you could save this to its own df so that you could then export it to Excel for a key stakeholder.\nDesc_Data \u0026lt;- as.data.frame(describe(Data))\r\rRound describe and save as a DF\rThat’s pretty gnarly. Let’s see if we can round that to 3 decimal places.\nDesc_Data \u0026lt;- round(as.data.frame(describe(Data)),3)\rMuch better!\n\rWrite to Excel\rTo send it to an Excel file we simply call upon openxlsx with the write.xlsx function.\nopenxlsx::write.xlsx(Desc_Data, \u0026quot;00_Data/Descriptive Statistics for Data.xlsx\u0026quot;)\r\rAdd an ID column\rIf we want to add and ID column, we can use the mutate function from dplyr to create a new variable. Here we will create a new object called TEST just in case this doesn’t go according to plan.\nTEST \u0026lt;- Data %\u0026gt;%\rmutate(ID = row_number())\r## mutate: new variable \u0026#39;ID\u0026#39; (integer) with 1,470 unique values and 0% NA\rAnd we’ll take a look at our new DF.\nhead(TEST)\r## Age Attrition BusinessTravel DailyRate Department\r## 1 41 Yes Travel_Rarely 1102 Sales\r## 2 49 No Travel_Frequently 279 Research_Development\r## 3 37 Yes Travel_Rarely 1373 Research_Development\r## 4 33 No Travel_Frequently 1392 Research_Development\r## 5 27 No Travel_Rarely 591 Research_Development\r## 6 32 No Travel_Frequently 1005 Research_Development\r## DistanceFromHome Education EducationField EnvironmentSatisfaction Gender\r## 1 1 College Life_Sciences Medium Female\r## 2 8 Below_College Life_Sciences High Male\r## 3 2 College Other Very_High Male\r## 4 3 Master Life_Sciences Very_High Female\r## 5 2 Below_College Medical Low Male\r## 6 2 College Life_Sciences Very_High Male\r## HourlyRate JobInvolvement JobLevel JobRole JobSatisfaction\r## 1 94 High 2 Sales_Executive Very_High\r## 2 61 Medium 2 Research_Scientist Medium\r## 3 92 Medium 1 Laboratory_Technician High\r## 4 56 High 1 Research_Scientist High\r## 5 40 High 1 Laboratory_Technician Medium\r## 6 79 High 1 Laboratory_Technician Very_High\r## MaritalStatus MonthlyIncome MonthlyRate NumCompaniesWorked OverTime\r## 1 Single 5993 19479 8 Yes\r## 2 Married 5130 24907 1 No\r## 3 Single 2090 2396 6 Yes\r## 4 Married 2909 23159 1 Yes\r## 5 Married 3468 16632 9 No\r## 6 Single 3068 11864 0 No\r## PercentSalaryHike PerformanceRating RelationshipSatisfaction StockOptionLevel\r## 1 11 Excellent Low 0\r## 2 23 Outstanding Very_High 1\r## 3 15 Excellent Medium 0\r## 4 11 Excellent High 0\r## 5 12 Excellent Very_High 1\r## 6 13 Excellent High 0\r## TotalWorkingYears TrainingTimesLastYear WorkLifeBalance YearsAtCompany\r## 1 8 0 Bad 6\r## 2 10 3 Better 10\r## 3 7 3 Better 0\r## 4 8 3 Better 8\r## 5 6 3 Better 2\r## 6 8 2 Good 7\r## YearsInCurrentRole YearsSinceLastPromotion YearsWithCurrManager ID\r## 1 4 0 5 1\r## 2 7 1 7 2\r## 3 0 0 0 3\r## 4 7 3 0 4\r## 5 2 2 2 5\r## 6 7 3 6 6\rIt has added it to the very end. We would like to put this as the first column. There are a few ways to do this. Using the select function, we can name the ID column we just created and use the everything function to tell it to put ID first and then put everything after ID.\nTEST \u0026lt;- TEST %\u0026gt;%\rselect(ID, everything())\r## select: columns reordered (ID, Age, Attrition, BusinessTravel, DailyRate, …)\rhead(TEST)\r## ID Age Attrition BusinessTravel DailyRate Department\r## 1 1 41 Yes Travel_Rarely 1102 Sales\r## 2 2 49 No Travel_Frequently 279 Research_Development\r## 3 3 37 Yes Travel_Rarely 1373 Research_Development\r## 4 4 33 No Travel_Frequently 1392 Research_Development\r## 5 5 27 No Travel_Rarely 591 Research_Development\r## 6 6 32 No Travel_Frequently 1005 Research_Development\r## DistanceFromHome Education EducationField EnvironmentSatisfaction Gender\r## 1 1 College Life_Sciences Medium Female\r## 2 8 Below_College Life_Sciences High Male\r## 3 2 College Other Very_High Male\r## 4 3 Master Life_Sciences Very_High Female\r## 5 2 Below_College Medical Low Male\r## 6 2 College Life_Sciences Very_High Male\r## HourlyRate JobInvolvement JobLevel JobRole JobSatisfaction\r## 1 94 High 2 Sales_Executive Very_High\r## 2 61 Medium 2 Research_Scientist Medium\r## 3 92 Medium 1 Laboratory_Technician High\r## 4 56 High 1 Research_Scientist High\r## 5 40 High 1 Laboratory_Technician Medium\r## 6 79 High 1 Laboratory_Technician Very_High\r## MaritalStatus MonthlyIncome MonthlyRate NumCompaniesWorked OverTime\r## 1 Single 5993 19479 8 Yes\r## 2 Married 5130 24907 1 No\r## 3 Single 2090 2396 6 Yes\r## 4 Married 2909 23159 1 Yes\r## 5 Married 3468 16632 9 No\r## 6 Single 3068 11864 0 No\r## PercentSalaryHike PerformanceRating RelationshipSatisfaction StockOptionLevel\r## 1 11 Excellent Low 0\r## 2 23 Outstanding Very_High 1\r## 3 15 Excellent Medium 0\r## 4 11 Excellent High 0\r## 5 12 Excellent Very_High 1\r## 6 13 Excellent High 0\r## TotalWorkingYears TrainingTimesLastYear WorkLifeBalance YearsAtCompany\r## 1 8 0 Bad 6\r## 2 10 3 Better 10\r## 3 7 3 Better 0\r## 4 8 3 Better 8\r## 5 6 3 Better 2\r## 6 8 2 Good 7\r## YearsInCurrentRole YearsSinceLastPromotion YearsWithCurrManager\r## 1 4 0 5\r## 2 7 1 7\r## 3 0 0 0\r## 4 7 3 0\r## 5 2 2 2\r## 6 7 3 6\rWe could have also used the new relocate function from dplyr to do the same thing and tell it to put ID before Age.\nTEST \u0026lt;- TEST %\u0026gt;%\rrelocate(ID, .before = Age)\r## relocate: no changes\rhead(TEST)\r## ID Age Attrition BusinessTravel DailyRate Department\r## 1 1 41 Yes Travel_Rarely 1102 Sales\r## 2 2 49 No Travel_Frequently 279 Research_Development\r## 3 3 37 Yes Travel_Rarely 1373 Research_Development\r## 4 4 33 No Travel_Frequently 1392 Research_Development\r## 5 5 27 No Travel_Rarely 591 Research_Development\r## 6 6 32 No Travel_Frequently 1005 Research_Development\r## DistanceFromHome Education EducationField EnvironmentSatisfaction Gender\r## 1 1 College Life_Sciences Medium Female\r## 2 8 Below_College Life_Sciences High Male\r## 3 2 College Other Very_High Male\r## 4 3 Master Life_Sciences Very_High Female\r## 5 2 Below_College Medical Low Male\r## 6 2 College Life_Sciences Very_High Male\r## HourlyRate JobInvolvement JobLevel JobRole JobSatisfaction\r## 1 94 High 2 Sales_Executive Very_High\r## 2 61 Medium 2 Research_Scientist Medium\r## 3 92 Medium 1 Laboratory_Technician High\r## 4 56 High 1 Research_Scientist High\r## 5 40 High 1 Laboratory_Technician Medium\r## 6 79 High 1 Laboratory_Technician Very_High\r## MaritalStatus MonthlyIncome MonthlyRate NumCompaniesWorked OverTime\r## 1 Single 5993 19479 8 Yes\r## 2 Married 5130 24907 1 No\r## 3 Single 2090 2396 6 Yes\r## 4 Married 2909 23159 1 Yes\r## 5 Married 3468 16632 9 No\r## 6 Single 3068 11864 0 No\r## PercentSalaryHike PerformanceRating RelationshipSatisfaction StockOptionLevel\r## 1 11 Excellent Low 0\r## 2 23 Outstanding Very_High 1\r## 3 15 Excellent Medium 0\r## 4 11 Excellent High 0\r## 5 12 Excellent Very_High 1\r## 6 13 Excellent High 0\r## TotalWorkingYears TrainingTimesLastYear WorkLifeBalance YearsAtCompany\r## 1 8 0 Bad 6\r## 2 10 3 Better 10\r## 3 7 3 Better 0\r## 4 8 3 Better 8\r## 5 6 3 Better 2\r## 6 8 2 Good 7\r## YearsInCurrentRole YearsSinceLastPromotion YearsWithCurrManager\r## 1 4 0 5\r## 2 7 1 7\r## 3 0 0 0\r## 4 7 3 0\r## 5 2 2 2\r## 6 7 3 6\rNow that we know this is going to work we can simply save Data as the updated Data object with our new ID variable located before Age like so.\nData \u0026lt;- Data %\u0026gt;% mutate(ID = row_number()) %\u0026gt;%\rrelocate(ID, .before = Age)\r## mutate: new variable \u0026#39;ID\u0026#39; (integer) with 1,470 unique values and 0% NA\r## relocate: columns reordered (ID, Age, Attrition, BusinessTravel, DailyRate, …)\rhead(Data)\r## ID Age Attrition BusinessTravel DailyRate Department\r## 1 1 41 Yes Travel_Rarely 1102 Sales\r## 2 2 49 No Travel_Frequently 279 Research_Development\r## 3 3 37 Yes Travel_Rarely 1373 Research_Development\r## 4 4 33 No Travel_Frequently 1392 Research_Development\r## 5 5 27 No Travel_Rarely 591 Research_Development\r## 6 6 32 No Travel_Frequently 1005 Research_Development\r## DistanceFromHome Education EducationField EnvironmentSatisfaction Gender\r## 1 1 College Life_Sciences Medium Female\r## 2 8 Below_College Life_Sciences High Male\r## 3 2 College Other Very_High Male\r## 4 3 Master Life_Sciences Very_High Female\r## 5 2 Below_College Medical Low Male\r## 6 2 College Life_Sciences Very_High Male\r## HourlyRate JobInvolvement JobLevel JobRole JobSatisfaction\r## 1 94 High 2 Sales_Executive Very_High\r## 2 61 Medium 2 Research_Scientist Medium\r## 3 92 Medium 1 Laboratory_Technician High\r## 4 56 High 1 Research_Scientist High\r## 5 40 High 1 Laboratory_Technician Medium\r## 6 79 High 1 Laboratory_Technician Very_High\r## MaritalStatus MonthlyIncome MonthlyRate NumCompaniesWorked OverTime\r## 1 Single 5993 19479 8 Yes\r## 2 Married 5130 24907 1 No\r## 3 Single 2090 2396 6 Yes\r## 4 Married 2909 23159 1 Yes\r## 5 Married 3468 16632 9 No\r## 6 Single 3068 11864 0 No\r## PercentSalaryHike PerformanceRating RelationshipSatisfaction StockOptionLevel\r## 1 11 Excellent Low 0\r## 2 23 Outstanding Very_High 1\r## 3 15 Excellent Medium 0\r## 4 11 Excellent High 0\r## 5 12 Excellent Very_High 1\r## 6 13 Excellent High 0\r## TotalWorkingYears TrainingTimesLastYear WorkLifeBalance YearsAtCompany\r## 1 8 0 Bad 6\r## 2 10 3 Better 10\r## 3 7 3 Better 0\r## 4 8 3 Better 8\r## 5 6 3 Better 2\r## 6 8 2 Good 7\r## YearsInCurrentRole YearsSinceLastPromotion YearsWithCurrManager\r## 1 4 0 5\r## 2 7 1 7\r## 3 0 0 0\r## 4 7 3 0\r## 5 2 2 2\r## 6 7 3 6\rNotice we did both of those steps together. To do this we used a pipe, %\u0026gt;%. Pipes are very useful for string steps of a code together in an easy to read and easy to follow fashion. I typically pronounce %\u0026gt;% as “then”. So, we mutated our new ID variable to correspond to row number then we relocated our ID variable to be before Age.\n\r\rCharacter Data\rNow we will get to know the character variables a little bit better.\nI learned the following methodology from Matt Dancho’s (@mdancho84) data science class at business-science.io. If you are interested in applying R to business problems, I highly recommend you check his classes out.\nWe’re now going to use some more of our dplyr verbs. First up is select_if. This makes it easier to select certain types of data. In this case, it will be character data. We can pair it with is.character and pipe in glimpse.\nData %\u0026gt;%\rselect_if(is.character) %\u0026gt;%\rglimpse()\r## select_if: dropped all variables\r## Rows: 1,470\r## Columns: 0\rIf you’ve enabled tidylog, you see that select_if: dropped all variables. I forgot that the built in version of the Attrition data had already been factorized. Let’s try again and swap out is.factor for is.character.\nData %\u0026gt;%\rselect_if(is.factor) %\u0026gt;%\rglimpse()\r## select_if: dropped 17 variables (ID, Age, DailyRate, DistanceFromHome, HourlyRate, …)\r## Rows: 1,470\r## Columns: 15\r## $ Attrition \u0026lt;fct\u0026gt; Yes, No, Yes, No, No, No, No, No, No, No, ...\r## $ BusinessTravel \u0026lt;fct\u0026gt; Travel_Rarely, Travel_Frequently, Travel_R...\r## $ Department \u0026lt;fct\u0026gt; Sales, Research_Development, Research_Deve...\r## $ Education \u0026lt;ord\u0026gt; College, Below_College, College, Master, B...\r## $ EducationField \u0026lt;fct\u0026gt; Life_Sciences, Life_Sciences, Other, Life_...\r## $ EnvironmentSatisfaction \u0026lt;ord\u0026gt; Medium, High, Very_High, Very_High, Low, V...\r## $ Gender \u0026lt;fct\u0026gt; Female, Male, Male, Female, Male, Male, Fe...\r## $ JobInvolvement \u0026lt;ord\u0026gt; High, Medium, Medium, High, High, High, Ve...\r## $ JobRole \u0026lt;fct\u0026gt; Sales_Executive, Research_Scientist, Labor...\r## $ JobSatisfaction \u0026lt;ord\u0026gt; Very_High, Medium, High, High, Medium, Ver...\r## $ MaritalStatus \u0026lt;fct\u0026gt; Single, Married, Single, Married, Married,...\r## $ OverTime \u0026lt;fct\u0026gt; Yes, No, Yes, Yes, No, No, Yes, No, No, No...\r## $ PerformanceRating \u0026lt;ord\u0026gt; Excellent, Outstanding, Excellent, Excelle...\r## $ RelationshipSatisfaction \u0026lt;ord\u0026gt; Low, Very_High, Medium, High, Very_High, H...\r## $ WorkLifeBalance \u0026lt;ord\u0026gt; Bad, Better, Better, Better, Better, Good,...\rMuch better! We can now see more information about the 15 columns it returned.\nNow let us take a look at the unique variables within each column. We will call up the map feature from purrr.\nData %\u0026gt;%\rselect_if(is.factor) %\u0026gt;%\rmap(unique) #from purrr\r## select_if: dropped 17 variables (ID, Age, DailyRate, DistanceFromHome, HourlyRate, …)\r## $Attrition\r## [1] Yes No ## Levels: No Yes\r## ## $BusinessTravel\r## [1] Travel_Rarely Travel_Frequently Non-Travel ## Levels: Non-Travel Travel_Frequently Travel_Rarely\r## ## $Department\r## [1] Sales Research_Development Human_Resources ## Levels: Human_Resources Research_Development Sales\r## ## $Education\r## [1] College Below_College Master Bachelor Doctor ## Levels: Below_College \u0026lt; College \u0026lt; Bachelor \u0026lt; Master \u0026lt; Doctor\r## ## $EducationField\r## [1] Life_Sciences Other Medical Marketing ## [5] Technical_Degree Human_Resources ## 6 Levels: Human_Resources Life_Sciences Marketing Medical ... Technical_Degree\r## ## $EnvironmentSatisfaction\r## [1] Medium High Very_High Low ## Levels: Low \u0026lt; Medium \u0026lt; High \u0026lt; Very_High\r## ## $Gender\r## [1] Female Male ## Levels: Female Male\r## ## $JobInvolvement\r## [1] High Medium Very_High Low ## Levels: Low \u0026lt; Medium \u0026lt; High \u0026lt; Very_High\r## ## $JobRole\r## [1] Sales_Executive Research_Scientist ## [3] Laboratory_Technician Manufacturing_Director ## [5] Healthcare_Representative Manager ## [7] Sales_Representative Research_Director ## [9] Human_Resources ## 9 Levels: Healthcare_Representative Human_Resources ... Sales_Representative\r## ## $JobSatisfaction\r## [1] Very_High Medium High Low ## Levels: Low \u0026lt; Medium \u0026lt; High \u0026lt; Very_High\r## ## $MaritalStatus\r## [1] Single Married Divorced\r## Levels: Divorced Married Single\r## ## $OverTime\r## [1] Yes No ## Levels: No Yes\r## ## $PerformanceRating\r## [1] Excellent Outstanding\r## Levels: Low \u0026lt; Good \u0026lt; Excellent \u0026lt; Outstanding\r## ## $RelationshipSatisfaction\r## [1] Low Very_High Medium High ## Levels: Low \u0026lt; Medium \u0026lt; High \u0026lt; Very_High\r## ## $WorkLifeBalance\r## [1] Bad Better Good Best ## Levels: Bad \u0026lt; Good \u0026lt; Better \u0026lt; Best\rSince these are factors, in addition to the unique values, it also gives us the levels. Notice WorkLifeBalance at the very bottom and the levels of Bad \u0026lt; Good \u0026lt; Better \u0026lt; Best. This is showing that Bad is less than Good which is less than Better and so on. We’ll revisit how to do this in a later post as the data almost never hits your inbox with this information included.\nAnother way to look at the data is to use the table function. We will swap out unique for table within map.\nData %\u0026gt;%\rselect_if(is.factor) %\u0026gt;%\rmap(table)\r## select_if: dropped 17 variables (ID, Age, DailyRate, DistanceFromHome, HourlyRate, …)\r## $Attrition\r## ## No Yes ## 1233 237 ## ## $BusinessTravel\r## ## Non-Travel Travel_Frequently Travel_Rarely ## 150 277 1043 ## ## $Department\r## ## Human_Resources Research_Development Sales ## 63 961 446 ## ## $Education\r## ## Below_College College Bachelor Master Doctor ## 170 282 572 398 48 ## ## $EducationField\r## ## Human_Resources Life_Sciences Marketing Medical ## 27 606 159 464 ## Other Technical_Degree ## 82 132 ## ## $EnvironmentSatisfaction\r## ## Low Medium High Very_High ## 284 287 453 446 ## ## $Gender\r## ## Female Male ## 588 882 ## ## $JobInvolvement\r## ## Low Medium High Very_High ## 83 375 868 144 ## ## $JobRole\r## ## Healthcare_Representative Human_Resources Laboratory_Technician ## 131 52 259 ## Manager Manufacturing_Director Research_Director ## 102 145 80 ## Research_Scientist Sales_Executive Sales_Representative ## 292 326 83 ## ## $JobSatisfaction\r## ## Low Medium High Very_High ## 289 280 442 459 ## ## $MaritalStatus\r## ## Divorced Married Single ## 327 673 470 ## ## $OverTime\r## ## No Yes ## 1054 416 ## ## $PerformanceRating\r## ## Low Good Excellent Outstanding ## 0 0 1244 226 ## ## $RelationshipSatisfaction\r## ## Low Medium High Very_High ## 276 303 459 432 ## ## $WorkLifeBalance\r## ## Bad Good Better Best ## 80 344 893 153\rThis way of looking at the data is very helpful since you can begin to get a feel for the factors. Performance rating has 0 for both Low and Good. This may be accurate or it may be some faulty data. Seeing this let’s you reach out to whoever would know and either check off on the data or realize a mistake has been made before you go further in your analysis.\nYou can also get proportions if that helps you to understand the data better. I won’t go too much into how this works here, but we may step through this a bit more in a future post if you are interested in doing so.\n# To get proportions\rData %\u0026gt;%\rselect_if(is.factor) %\u0026gt;%\rmap(~ table(.) %\u0026gt;% prop.table()) #anonymous function\r## select_if: dropped 17 variables (ID, Age, DailyRate, DistanceFromHome, HourlyRate, …)\r## $Attrition\r## .\r## No Yes ## 0.8387755 0.1612245 ## ## $BusinessTravel\r## .\r## Non-Travel Travel_Frequently Travel_Rarely ## 0.1020408 0.1884354 0.7095238 ## ## $Department\r## .\r## Human_Resources Research_Development Sales ## 0.04285714 0.65374150 0.30340136 ## ## $Education\r## .\r## Below_College College Bachelor Master Doctor ## 0.11564626 0.19183673 0.38911565 0.27074830 0.03265306 ## ## $EducationField\r## .\r## Human_Resources Life_Sciences Marketing Medical ## 0.01836735 0.41224490 0.10816327 0.31564626 ## Other Technical_Degree ## 0.05578231 0.08979592 ## ## $EnvironmentSatisfaction\r## .\r## Low Medium High Very_High ## 0.1931973 0.1952381 0.3081633 0.3034014 ## ## $Gender\r## .\r## Female Male ## 0.4 0.6 ## ## $JobInvolvement\r## .\r## Low Medium High Very_High ## 0.05646259 0.25510204 0.59047619 0.09795918 ## ## $JobRole\r## .\r## Healthcare_Representative Human_Resources Laboratory_Technician ## 0.08911565 0.03537415 0.17619048 ## Manager Manufacturing_Director Research_Director ## 0.06938776 0.09863946 0.05442177 ## Research_Scientist Sales_Executive Sales_Representative ## 0.19863946 0.22176871 0.05646259 ## ## $JobSatisfaction\r## .\r## Low Medium High Very_High ## 0.1965986 0.1904762 0.3006803 0.3122449 ## ## $MaritalStatus\r## .\r## Divorced Married Single ## 0.2224490 0.4578231 0.3197279 ## ## $OverTime\r## .\r## No Yes ## 0.7170068 0.2829932 ## ## $PerformanceRating\r## .\r## Low Good Excellent Outstanding ## 0.0000000 0.0000000 0.8462585 0.1537415 ## ## $RelationshipSatisfaction\r## .\r## Low Medium High Very_High ## 0.1877551 0.2061224 0.3122449 0.2938776 ## ## $WorkLifeBalance\r## .\r## Bad Good Better Best ## 0.05442177 0.23401361 0.60748299 0.10408163\rThis now gives us the proportions, but I struggle with anything more than about 2 or 3 decimals. Let’s go ahead and clean this up a bit. We have wrapped everything in the round function. Also, notice that I didn’t always know how to do this. I’ve put where I learned how do to this as a URL I’ve commented out from stackoverflow.com. Remember to leave yourself breadcrumbs to find your way home! You may not need these breadcrumbs tomorrow, but you will thank yourself when you have to come back to your code 6 months or more down the road. I also think the breadcrumbs are helpful if you have to share your code.\n# To get proportions\r# Rounded (From: https://stackoverflow.com/questions/43013016/how-to-create-multiple-frequency-tables-with-percentages-across-factor-variables)\rData %\u0026gt;%\rselect_if(is.factor) %\u0026gt;%\rmap(~ round(table(.) %\u0026gt;% prop.table(), 2)) #anonymous function\r## select_if: dropped 17 variables (ID, Age, DailyRate, DistanceFromHome, HourlyRate, …)\r## $Attrition\r## .\r## No Yes ## 0.84 0.16 ## ## $BusinessTravel\r## .\r## Non-Travel Travel_Frequently Travel_Rarely ## 0.10 0.19 0.71 ## ## $Department\r## .\r## Human_Resources Research_Development Sales ## 0.04 0.65 0.30 ## ## $Education\r## .\r## Below_College College Bachelor Master Doctor ## 0.12 0.19 0.39 0.27 0.03 ## ## $EducationField\r## .\r## Human_Resources Life_Sciences Marketing Medical ## 0.02 0.41 0.11 0.32 ## Other Technical_Degree ## 0.06 0.09 ## ## $EnvironmentSatisfaction\r## .\r## Low Medium High Very_High ## 0.19 0.20 0.31 0.30 ## ## $Gender\r## .\r## Female Male ## 0.4 0.6 ## ## $JobInvolvement\r## .\r## Low Medium High Very_High ## 0.06 0.26 0.59 0.10 ## ## $JobRole\r## .\r## Healthcare_Representative Human_Resources Laboratory_Technician ## 0.09 0.04 0.18 ## Manager Manufacturing_Director Research_Director ## 0.07 0.10 0.05 ## Research_Scientist Sales_Executive Sales_Representative ## 0.20 0.22 0.06 ## ## $JobSatisfaction\r## .\r## Low Medium High Very_High ## 0.20 0.19 0.30 0.31 ## ## $MaritalStatus\r## .\r## Divorced Married Single ## 0.22 0.46 0.32 ## ## $OverTime\r## .\r## No Yes ## 0.72 0.28 ## ## $PerformanceRating\r## .\r## Low Good Excellent Outstanding ## 0.00 0.00 0.85 0.15 ## ## $RelationshipSatisfaction\r## .\r## Low Medium High Very_High ## 0.19 0.21 0.31 0.29 ## ## $WorkLifeBalance\r## .\r## Bad Good Better Best ## 0.05 0.23 0.61 0.10\rRounded to 2 decimal places looks much better to my eye. If 3 looks better to yours, simply swap out the 2 for a 3.\n# To get proportions\r# Rounded (From: https://stackoverflow.com/questions/43013016/how-to-create-multiple-frequency-tables-with-percentages-across-factor-variables)\rData %\u0026gt;%\rselect_if(is.factor) %\u0026gt;%\rmap(~ round(table(.) %\u0026gt;% prop.table(), 3)) #anonymous function\r## select_if: dropped 17 variables (ID, Age, DailyRate, DistanceFromHome, HourlyRate, …)\r## $Attrition\r## .\r## No Yes ## 0.839 0.161 ## ## $BusinessTravel\r## .\r## Non-Travel Travel_Frequently Travel_Rarely ## 0.102 0.188 0.710 ## ## $Department\r## .\r## Human_Resources Research_Development Sales ## 0.043 0.654 0.303 ## ## $Education\r## .\r## Below_College College Bachelor Master Doctor ## 0.116 0.192 0.389 0.271 0.033 ## ## $EducationField\r## .\r## Human_Resources Life_Sciences Marketing Medical ## 0.018 0.412 0.108 0.316 ## Other Technical_Degree ## 0.056 0.090 ## ## $EnvironmentSatisfaction\r## .\r## Low Medium High Very_High ## 0.193 0.195 0.308 0.303 ## ## $Gender\r## .\r## Female Male ## 0.4 0.6 ## ## $JobInvolvement\r## .\r## Low Medium High Very_High ## 0.056 0.255 0.590 0.098 ## ## $JobRole\r## .\r## Healthcare_Representative Human_Resources Laboratory_Technician ## 0.089 0.035 0.176 ## Manager Manufacturing_Director Research_Director ## 0.069 0.099 0.054 ## Research_Scientist Sales_Executive Sales_Representative ## 0.199 0.222 0.056 ## ## $JobSatisfaction\r## .\r## Low Medium High Very_High ## 0.197 0.190 0.301 0.312 ## ## $MaritalStatus\r## .\r## Divorced Married Single ## 0.222 0.458 0.320 ## ## $OverTime\r## .\r## No Yes ## 0.717 0.283 ## ## $PerformanceRating\r## .\r## Low Good Excellent Outstanding ## 0.000 0.000 0.846 0.154 ## ## $RelationshipSatisfaction\r## .\r## Low Medium High Very_High ## 0.188 0.206 0.312 0.294 ## ## $WorkLifeBalance\r## .\r## Bad Good Better Best ## 0.054 0.234 0.607 0.104\rRemember, there are also usually multiple ways of doing things in R. We could have accomplished the same thing with the following code. I say choose whichever coding style looks better to you.\n# To get proportions\r# Rounded (From: https://stackoverflow.com/questions/43013016/how-to-create-multiple-frequency-tables-with-percentages-across-factor-variables)\rData %\u0026gt;%\rselect_if(is.factor) %\u0026gt;%\rmap(~round(prop.table(table(.x)),2))\r## select_if: dropped 17 variables (ID, Age, DailyRate, DistanceFromHome, HourlyRate, …)\r## $Attrition\r## .x\r## No Yes ## 0.84 0.16 ## ## $BusinessTravel\r## .x\r## Non-Travel Travel_Frequently Travel_Rarely ## 0.10 0.19 0.71 ## ## $Department\r## .x\r## Human_Resources Research_Development Sales ## 0.04 0.65 0.30 ## ## $Education\r## .x\r## Below_College College Bachelor Master Doctor ## 0.12 0.19 0.39 0.27 0.03 ## ## $EducationField\r## .x\r## Human_Resources Life_Sciences Marketing Medical ## 0.02 0.41 0.11 0.32 ## Other Technical_Degree ## 0.06 0.09 ## ## $EnvironmentSatisfaction\r## .x\r## Low Medium High Very_High ## 0.19 0.20 0.31 0.30 ## ## $Gender\r## .x\r## Female Male ## 0.4 0.6 ## ## $JobInvolvement\r## .x\r## Low Medium High Very_High ## 0.06 0.26 0.59 0.10 ## ## $JobRole\r## .x\r## Healthcare_Representative Human_Resources Laboratory_Technician ## 0.09 0.04 0.18 ## Manager Manufacturing_Director Research_Director ## 0.07 0.10 0.05 ## Research_Scientist Sales_Executive Sales_Representative ## 0.20 0.22 0.06 ## ## $JobSatisfaction\r## .x\r## Low Medium High Very_High ## 0.20 0.19 0.30 0.31 ## ## $MaritalStatus\r## .x\r## Divorced Married Single ## 0.22 0.46 0.32 ## ## $OverTime\r## .x\r## No Yes ## 0.72 0.28 ## ## $PerformanceRating\r## .x\r## Low Good Excellent Outstanding ## 0.00 0.00 0.85 0.15 ## ## $RelationshipSatisfaction\r## .x\r## Low Medium High Very_High ## 0.19 0.21 0.31 0.29 ## ## $WorkLifeBalance\r## .x\r## Bad Good Better Best ## 0.05 0.23 0.61 0.10\rWe could have also utilized the tabyl function from janitor, but now I’m going down a bit of a rabbit hole. The point is, mess around with the code and the data. Make it your own.\nData %\u0026gt;%\rselect_if(is.factor) %\u0026gt;%\rmap(tabyl)\r## select_if: dropped 17 variables (ID, Age, DailyRate, DistanceFromHome, HourlyRate, …)\r## $Attrition\r## .x[[i]] n percent\r## No 1233 0.8387755\r## Yes 237 0.1612245\r## ## $BusinessTravel\r## .x[[i]] n percent\r## Non-Travel 150 0.1020408\r## Travel_Frequently 277 0.1884354\r## Travel_Rarely 1043 0.7095238\r## ## $Department\r## .x[[i]] n percent\r## Human_Resources 63 0.04285714\r## Research_Development 961 0.65374150\r## Sales 446 0.30340136\r## ## $Education\r## .x[[i]] n percent\r## Below_College 170 0.11564626\r## College 282 0.19183673\r## Bachelor 572 0.38911565\r## Master 398 0.27074830\r## Doctor 48 0.03265306\r## ## $EducationField\r## .x[[i]] n percent\r## Human_Resources 27 0.01836735\r## Life_Sciences 606 0.41224490\r## Marketing 159 0.10816327\r## Medical 464 0.31564626\r## Other 82 0.05578231\r## Technical_Degree 132 0.08979592\r## ## $EnvironmentSatisfaction\r## .x[[i]] n percent\r## Low 284 0.1931973\r## Medium 287 0.1952381\r## High 453 0.3081633\r## Very_High 446 0.3034014\r## ## $Gender\r## .x[[i]] n percent\r## Female 588 0.4\r## Male 882 0.6\r## ## $JobInvolvement\r## .x[[i]] n percent\r## Low 83 0.05646259\r## Medium 375 0.25510204\r## High 868 0.59047619\r## Very_High 144 0.09795918\r## ## $JobRole\r## .x[[i]] n percent\r## Healthcare_Representative 131 0.08911565\r## Human_Resources 52 0.03537415\r## Laboratory_Technician 259 0.17619048\r## Manager 102 0.06938776\r## Manufacturing_Director 145 0.09863946\r## Research_Director 80 0.05442177\r## Research_Scientist 292 0.19863946\r## Sales_Executive 326 0.22176871\r## Sales_Representative 83 0.05646259\r## ## $JobSatisfaction\r## .x[[i]] n percent\r## Low 289 0.1965986\r## Medium 280 0.1904762\r## High 442 0.3006803\r## Very_High 459 0.3122449\r## ## $MaritalStatus\r## .x[[i]] n percent\r## Divorced 327 0.2224490\r## Married 673 0.4578231\r## Single 470 0.3197279\r## ## $OverTime\r## .x[[i]] n percent\r## No 1054 0.7170068\r## Yes 416 0.2829932\r## ## $PerformanceRating\r## .x[[i]] n percent\r## Low 0 0.0000000\r## Good 0 0.0000000\r## Excellent 1244 0.8462585\r## Outstanding 226 0.1537415\r## ## $RelationshipSatisfaction\r## .x[[i]] n percent\r## Low 276 0.1877551\r## Medium 303 0.2061224\r## High 459 0.3122449\r## Very_High 432 0.2938776\r## ## $WorkLifeBalance\r## .x[[i]] n percent\r## Bad 80 0.05442177\r## Good 344 0.23401361\r## Better 893 0.60748299\r## Best 153 0.10408163\rData %\u0026gt;%\rselect_if(is.factor) %\u0026gt;%\rmap(tabyl)\r## select_if: dropped 17 variables (ID, Age, DailyRate, DistanceFromHome, HourlyRate, …)\r## $Attrition\r## .x[[i]] n percent\r## No 1233 0.8387755\r## Yes 237 0.1612245\r## ## $BusinessTravel\r## .x[[i]] n percent\r## Non-Travel 150 0.1020408\r## Travel_Frequently 277 0.1884354\r## Travel_Rarely 1043 0.7095238\r## ## $Department\r## .x[[i]] n percent\r## Human_Resources 63 0.04285714\r## Research_Development 961 0.65374150\r## Sales 446 0.30340136\r## ## $Education\r## .x[[i]] n percent\r## Below_College 170 0.11564626\r## College 282 0.19183673\r## Bachelor 572 0.38911565\r## Master 398 0.27074830\r## Doctor 48 0.03265306\r## ## $EducationField\r## .x[[i]] n percent\r## Human_Resources 27 0.01836735\r## Life_Sciences 606 0.41224490\r## Marketing 159 0.10816327\r## Medical 464 0.31564626\r## Other 82 0.05578231\r## Technical_Degree 132 0.08979592\r## ## $EnvironmentSatisfaction\r## .x[[i]] n percent\r## Low 284 0.1931973\r## Medium 287 0.1952381\r## High 453 0.3081633\r## Very_High 446 0.3034014\r## ## $Gender\r## .x[[i]] n percent\r## Female 588 0.4\r## Male 882 0.6\r## ## $JobInvolvement\r## .x[[i]] n percent\r## Low 83 0.05646259\r## Medium 375 0.25510204\r## High 868 0.59047619\r## Very_High 144 0.09795918\r## ## $JobRole\r## .x[[i]] n percent\r## Healthcare_Representative 131 0.08911565\r## Human_Resources 52 0.03537415\r## Laboratory_Technician 259 0.17619048\r## Manager 102 0.06938776\r## Manufacturing_Director 145 0.09863946\r## Research_Director 80 0.05442177\r## Research_Scientist 292 0.19863946\r## Sales_Executive 326 0.22176871\r## Sales_Representative 83 0.05646259\r## ## $JobSatisfaction\r## .x[[i]] n percent\r## Low 289 0.1965986\r## Medium 280 0.1904762\r## High 442 0.3006803\r## Very_High 459 0.3122449\r## ## $MaritalStatus\r## .x[[i]] n percent\r## Divorced 327 0.2224490\r## Married 673 0.4578231\r## Single 470 0.3197279\r## ## $OverTime\r## .x[[i]] n percent\r## No 1054 0.7170068\r## Yes 416 0.2829932\r## ## $PerformanceRating\r## .x[[i]] n percent\r## Low 0 0.0000000\r## Good 0 0.0000000\r## Excellent 1244 0.8462585\r## Outstanding 226 0.1537415\r## ## $RelationshipSatisfaction\r## .x[[i]] n percent\r## Low 276 0.1877551\r## Medium 303 0.2061224\r## High 459 0.3122449\r## Very_High 432 0.2938776\r## ## $WorkLifeBalance\r## .x[[i]] n percent\r## Bad 80 0.05442177\r## Good 344 0.23401361\r## Better 893 0.60748299\r## Best 153 0.10408163\r# Data %\u0026gt;%\r# select_if(is.factor) %\u0026gt;%\r# map(~ round(table(.) %\u0026gt;% prop.table(), 3)) #anonymous function\r\rNumeric Data\rI also enjoy Matt Dancho’s way of looking at numeric data so I wanted to include this. The code below should look pretty familiar. We are just swapping out is.numeric for is.factor. Adding length simply tells us how many unique values are in each numeric variable. So, as expected, we see 1470 for ID which makes sense because we have 1470 rows.\nData %\u0026gt;%\rselect_if(is.numeric) %\u0026gt;%\rmap(~ unique(.) %\u0026gt;% length())\r## select_if: dropped 15 variables (Attrition, BusinessTravel, Department, Education, EducationField, …)\r## $ID\r## [1] 1470\r## ## $Age\r## [1] 43\r## ## $DailyRate\r## [1] 886\r## ## $DistanceFromHome\r## [1] 29\r## ## $HourlyRate\r## [1] 71\r## ## $JobLevel\r## [1] 5\r## ## $MonthlyIncome\r## [1] 1349\r## ## $MonthlyRate\r## [1] 1427\r## ## $NumCompaniesWorked\r## [1] 10\r## ## $PercentSalaryHike\r## [1] 15\r## ## $StockOptionLevel\r## [1] 4\r## ## $TotalWorkingYears\r## [1] 40\r## ## $TrainingTimesLastYear\r## [1] 7\r## ## $YearsAtCompany\r## [1] 37\r## ## $YearsInCurrentRole\r## [1] 19\r## ## $YearsSinceLastPromotion\r## [1] 16\r## ## $YearsWithCurrManager\r## [1] 18\rChanging out map for map_df will turn this into a data frame. That makes it easier to export the file to something like Excel.\nData %\u0026gt;%\rselect_if(is.numeric) %\u0026gt;%\rmap_df(~ unique(.) %\u0026gt;% length()) # tries to turn it into a df instead of a list\r## select_if: dropped 15 variables (Attrition, BusinessTravel, Department, Education, EducationField, …)\r## # A tibble: 1 x 17\r## ID Age DailyRate DistanceFromHome HourlyRate JobLevel MonthlyIncome\r## \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt;\r## 1 1470 43 886 29 71 5 1349\r## # ... with 10 more variables: MonthlyRate \u0026lt;int\u0026gt;, NumCompaniesWorked \u0026lt;int\u0026gt;,\r## # PercentSalaryHike \u0026lt;int\u0026gt;, StockOptionLevel \u0026lt;int\u0026gt;, TotalWorkingYears \u0026lt;int\u0026gt;,\r## # TrainingTimesLastYear \u0026lt;int\u0026gt;, YearsAtCompany \u0026lt;int\u0026gt;,\r## # YearsInCurrentRole \u0026lt;int\u0026gt;, YearsSinceLastPromotion \u0026lt;int\u0026gt;,\r## # YearsWithCurrManager \u0026lt;int\u0026gt;\rThrowing gather after the pipe will make the data tall or pivot it.\nData %\u0026gt;%\rselect_if(is.numeric) %\u0026gt;%\rmap_df(~ unique(.) %\u0026gt;% length()) %\u0026gt;% # tries to turn it into a df instead of a list\rgather()\r## select_if: dropped 15 variables (Attrition, BusinessTravel, Department, Education, EducationField, …)\r## gather: reorganized (ID, Age, DailyRate, DistanceFromHome, HourlyRate, …) into (key, value) [was 1x17, now 17x2]\r## # A tibble: 17 x 2\r## key value\r## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt;\r## 1 ID 1470\r## 2 Age 43\r## 3 DailyRate 886\r## 4 DistanceFromHome 29\r## 5 HourlyRate 71\r## 6 JobLevel 5\r## 7 MonthlyIncome 1349\r## 8 MonthlyRate 1427\r## 9 NumCompaniesWorked 10\r## 10 PercentSalaryHike 15\r## 11 StockOptionLevel 4\r## 12 TotalWorkingYears 40\r## 13 TrainingTimesLastYear 7\r## 14 YearsAtCompany 37\r## 15 YearsInCurrentRole 19\r## 16 YearsSinceLastPromotion 16\r## 17 YearsWithCurrManager 18\rWe can pipe in arrange and set it to desc for descending to get the values from most to least.\nData %\u0026gt;%\rselect_if(is.numeric) %\u0026gt;%\rmap_df(~ unique(.) %\u0026gt;% length()) %\u0026gt;% # tries to turn it into a df instead of a list\rgather() %\u0026gt;%\rarrange(desc(value)) # Move the largest value to the top and go descending \r## select_if: dropped 15 variables (Attrition, BusinessTravel, Department, Education, EducationField, …)\r## gather: reorganized (ID, Age, DailyRate, DistanceFromHome, HourlyRate, …) into (key, value) [was 1x17, now 17x2]\r## # A tibble: 17 x 2\r## key value\r## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt;\r## 1 ID 1470\r## 2 MonthlyRate 1427\r## 3 MonthlyIncome 1349\r## 4 DailyRate 886\r## 5 HourlyRate 71\r## 6 Age 43\r## 7 TotalWorkingYears 40\r## 8 YearsAtCompany 37\r## 9 DistanceFromHome 29\r## 10 YearsInCurrentRole 19\r## 11 YearsWithCurrManager 18\r## 12 YearsSinceLastPromotion 16\r## 13 PercentSalaryHike 15\r## 14 NumCompaniesWorked 10\r## 15 TrainingTimesLastYear 7\r## 16 JobLevel 5\r## 17 StockOptionLevel 4\rNow it would be super easy to save it as a new object like Values_for_Excel or something like that to export with openxlsx::write.xlsx. We’ll cover that more in a later post.\nValues_for_Excel \u0026lt;- Data %\u0026gt;%\rselect_if(is.numeric) %\u0026gt;%\rmap_df(~ unique(.) %\u0026gt;% length()) %\u0026gt;% # tries to turn it into a df instead of a list\rgather() %\u0026gt;%\rarrange(desc(value)) # Move the largest value to the top and go descending \r## select_if: dropped 15 variables (Attrition, BusinessTravel, Department, Education, EducationField, …)\r## gather: reorganized (ID, Age, DailyRate, DistanceFromHome, HourlyRate, …) into (key, value) [was 1x17, now 17x2]\rValues_for_Excel now appears as a new df in your Global Environment.\nA neat trick I learned from Matt Dancho’s class is the following for determining if a numeric variables is more likely continuous or discrete.\nValues higher than 10 are probably continuous. PercentSalaryHike, YearsSinceLastPromotion, etc.\nData %\u0026gt;%\rselect_if(is.numeric) %\u0026gt;%\rmap_df(~ unique(.) %\u0026gt;% length()) %\u0026gt;% # tries to turn it into a df instead of a list\rgather() %\u0026gt;%\rarrange(value) %\u0026gt;%\rfilter(value \u0026gt; 10) #probably continuous if more than 10\r## select_if: dropped 15 variables (Attrition, BusinessTravel, Department, Education, EducationField, …)\r## gather: reorganized (ID, Age, DailyRate, DistanceFromHome, HourlyRate, …) into (key, value) [was 1x17, now 17x2]\r## # A tibble: 13 x 2\r## key value\r## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt;\r## 1 PercentSalaryHike 15\r## 2 YearsSinceLastPromotion 16\r## 3 YearsWithCurrManager 18\r## 4 YearsInCurrentRole 19\r## 5 DistanceFromHome 29\r## 6 YearsAtCompany 37\r## 7 TotalWorkingYears 40\r## 8 Age 43\r## 9 HourlyRate 71\r## 10 DailyRate 886\r## 11 MonthlyIncome 1349\r## 12 MonthlyRate 1427\r## 13 ID 1470\rValues less than 10 are probably discrete. JobLevel, NumCompaniesWorked, etc.\nData %\u0026gt;%\rselect_if(is.numeric) %\u0026gt;%\rmap_df(~ unique(.) %\u0026gt;% length()) %\u0026gt;% # tries to turn it into a df instead of a list\rgather() %\u0026gt;%\rarrange(value) %\u0026gt;%\rfilter(value \u0026lt;= 10) #probably discrete if less than 10\r## select_if: dropped 15 variables (Attrition, BusinessTravel, Department, Education, EducationField, …)\r## gather: reorganized (ID, Age, DailyRate, DistanceFromHome, HourlyRate, …) into (key, value) [was 1x17, now 17x2]\r## # A tibble: 4 x 2\r## key value\r## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt;\r## 1 StockOptionLevel 4\r## 2 JobLevel 5\r## 3 TrainingTimesLastYear 7\r## 4 NumCompaniesWorked 10\rYou may need to change 10 to be a little lower or higher to differentiate your specific data, but it is a great place to start.\n\rWrap up\rThat is it for now. Hopefully you have found something you can use here. Keep coming back as we will start to visualize the data and add to it as we try to make it more and more like something you will encounter in your work place.\n\rJust the code\rThanks to Yihui Xie and Lucy D’Agostino, I stumbled across this bit of sorcery using PURL where you simply include an R code chunk to save all of the above code in the appendix.\n#\u0026#39; \u0026lt;!-- ####################################################################################################### --\u0026gt;\r#\u0026#39; \u0026lt;!-- ####################################################################################################### --\u0026gt;\r#\u0026#39; \u0026lt;!-- ##################################LOADING PACKAGES##################################################### --\u0026gt;\rtryCatch(require(pacman),finally=utils:::install.packages(pkgs=\u0026#39;pacman\u0026#39;,repos=\u0026#39;http://cran.r-project.org\u0026#39;));\rrequire(pacman)\r# if the above doesn\u0026#39;t work, use this code#\r# tryCatch\r# detach(\u0026quot;package:pacman\u0026quot;, unload = TRUE)\r# install.packages(\u0026quot;pacman\u0026quot;, dependencies = TRUE)\r# install.packages(\u0026quot;pacman\u0026quot;)\rpacman::p_load(modeldata, # data sets useful for modeling packages\rdplyr, # a grammar of data manipulation\rtidyr, # tidy messy data\rreadxl, # read Excel files\rggplot2, # create elegant data visualisations using the grammar of graphics\rknitr, # a general-purpose package for dynamic report generation in R\rbroom, # convert statistical objects into tidy tibbles\rpurrr, # functional programming tools\rpsych, #procedures for psychological, psychometric, and personality research\rconflicted, # an alternative conflict resolution strategy for like named function in different libraries\rjanitor, # simple tools for examining and cleaning dirty data\rskimr, # compact and flexible summaries of data\ropenxlsx, # read write and edit xlsx files\rtidyquant, # tidy quantitative financial analysis\rtidylog # logging for `dplyr` and `tidyr` functions\r)\r#\u0026#39; \u0026lt;!-- #Loading libraries --\u0026gt;\rsuppressPackageStartupMessages({\rlibrary(modeldata) library(dplyr)\rlibrary(tidyr)\rlibrary(readxl)\rlibrary(ggplot2)\rlibrary(knitr)\rlibrary(broom)\rlibrary(purrr)\rlibrary(psych)\rlibrary(conflicted)\rlibrary(janitor)\rlibrary(skimr)\rlibrary(openxlsx)\rlibrary(tidyquant)\rlibrary(tidylog, warn.conflicts = FALSE)\r})\rfor (f in getNamespaceExports(\u0026quot;tidylog\u0026quot;)) {\rconflicted::conflict_prefer(f, \u0026quot;tidylog\u0026quot;, quiet = TRUE)\r}\rconflict_prefer(\u0026quot;filter\u0026quot;, \u0026quot;dplyr\u0026quot;)\rlibrary(modeldata)\rdata(attrition)\rData \u0026lt;- attrition\rlibrary(readxl)\rData \u0026lt;- read_excel(\u0026quot;D:/Folder where the file is located/WA_Fn-UseC_-HR-Employee-Attrition.xlsx\u0026quot;)\rhead(Data)\rhead(Data, n = 10)\rtail(Data)\rheadTail(Data)\rpsych::headTail(Data)\rdim(Data)\rstr(Data)\rglimpse(Data)\rskim(Data)\rdescribe(Data)\rDesc_Data \u0026lt;- as.data.frame(describe(Data))\rDesc_Data \u0026lt;- round(as.data.frame(describe(Data)),3)\ropenxlsx::write.xlsx(Desc_Data, \u0026quot;00_Data/Descriptive Statistics for Data.xlsx\u0026quot;)\rTEST \u0026lt;- Data %\u0026gt;%\rmutate(ID = row_number())\rhead(TEST)\rTEST \u0026lt;- TEST %\u0026gt;%\rselect(ID, everything())\rhead(TEST)\rTEST \u0026lt;- TEST %\u0026gt;%\rrelocate(ID, .before = Age)\rhead(TEST)\rData \u0026lt;- Data %\u0026gt;% mutate(ID = row_number()) %\u0026gt;%\rrelocate(ID, .before = Age)\rhead(Data)\rData %\u0026gt;%\rselect_if(is.character) %\u0026gt;%\rglimpse()\rData %\u0026gt;%\rselect_if(is.factor) %\u0026gt;%\rglimpse()\rData %\u0026gt;%\rselect_if(is.factor) %\u0026gt;%\rmap(unique) #from purrr\rData %\u0026gt;%\rselect_if(is.factor) %\u0026gt;%\rmap(table)\r# To get proportions\rData %\u0026gt;%\rselect_if(is.factor) %\u0026gt;%\rmap(~ table(.) %\u0026gt;% prop.table()) #anonymous function\r# To get proportions\r# Rounded (From: https://stackoverflow.com/questions/43013016/how-to-create-multiple-frequency-tables-with-percentages-across-factor-variables)\rData %\u0026gt;%\rselect_if(is.factor) %\u0026gt;%\rmap(~ round(table(.) %\u0026gt;% prop.table(), 2)) #anonymous function\r# To get proportions\r# Rounded (From: https://stackoverflow.com/questions/43013016/how-to-create-multiple-frequency-tables-with-percentages-across-factor-variables)\rData %\u0026gt;%\rselect_if(is.factor) %\u0026gt;%\rmap(~ round(table(.) %\u0026gt;% prop.table(), 3)) #anonymous function\r# To get proportions\r# Rounded (From: https://stackoverflow.com/questions/43013016/how-to-create-multiple-frequency-tables-with-percentages-across-factor-variables)\rData %\u0026gt;%\rselect_if(is.factor) %\u0026gt;%\rmap(~round(prop.table(table(.x)),2))\rData %\u0026gt;%\rselect_if(is.factor) %\u0026gt;%\rmap(tabyl)\rData %\u0026gt;%\rselect_if(is.factor) %\u0026gt;%\rmap(tabyl)\r# Data %\u0026gt;%\r# select_if(is.factor) %\u0026gt;%\r# map(~ round(table(.) %\u0026gt;% prop.table(), 3)) #anonymous function\rData %\u0026gt;%\rselect_if(is.numeric) %\u0026gt;%\rmap(~ unique(.) %\u0026gt;% length())\rData %\u0026gt;%\rselect_if(is.numeric) %\u0026gt;%\rmap_df(~ unique(.) %\u0026gt;% length()) # tries to turn it into a df instead of a list\rData %\u0026gt;%\rselect_if(is.numeric) %\u0026gt;%\rmap_df(~ unique(.) %\u0026gt;% length()) %\u0026gt;% # tries to turn it into a df instead of a list\rgather()\rData %\u0026gt;%\rselect_if(is.numeric) %\u0026gt;%\rmap_df(~ unique(.) %\u0026gt;% length()) %\u0026gt;% # tries to turn it into a df instead of a list\rgather() %\u0026gt;%\rarrange(desc(value)) # Move the largest value to the top and go descending Values_for_Excel \u0026lt;- Data %\u0026gt;%\rselect_if(is.numeric) %\u0026gt;%\rmap_df(~ unique(.) %\u0026gt;% length()) %\u0026gt;% # tries to turn it into a df instead of a list\rgather() %\u0026gt;%\rarrange(desc(value)) # Move the largest value to the top and go descending Data %\u0026gt;%\rselect_if(is.numeric) %\u0026gt;%\rmap_df(~ unique(.) %\u0026gt;% length()) %\u0026gt;% # tries to turn it into a df instead of a list\rgather() %\u0026gt;%\rarrange(value) %\u0026gt;%\rfilter(value \u0026gt; 10) #probably continuous if more than 10\rData %\u0026gt;%\rselect_if(is.numeric) %\u0026gt;%\rmap_df(~ unique(.) %\u0026gt;% length()) %\u0026gt;% # tries to turn it into a df instead of a list\rgather() %\u0026gt;%\rarrange(value) %\u0026gt;%\rfilter(value \u0026lt;= 10) #probably discrete if less than 10\r\r","date":1597968000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598035899,"objectID":"fb6cdd2c4bf33b84fbc9c1b5e0a52342","permalink":"http://robstilson.rbind.io/post/exploratory-data-analysis-part-1/","publishdate":"2020-08-21T00:00:00Z","relpermalink":"/post/exploratory-data-analysis-part-1/","section":"post","summary":"Summary\rGet Packages\rLoad packages\rBring in the data\rLoad data from Excel\rHead\rTail\rHeadTail\rDimensions of the data\rStructure of the data\rGlimpse the data\rSkim the data\rDescribe the data\rSave describe as a DF\rRound describe and save as a DF\rWrite to Excel\rAdd an ID column\r\rCharacter Data\rNumeric Data\rWrap up\rJust the code\r\r\rSummary\rIn this post I will introduce you to HR data from the IBM Watson Analytics Lab.","tags":[],"title":"Exploratory Data Analysis: Part 1","type":"post"},{"authors":[],"categories":[],"content":"\rWelcome!\rWelcome to the blog! My main purpose in creating this site is to show those doing I-O Psychology and Human Resources (HR) type work, the way that I use R, Python, and occasionally Tableau in my work.\nThis can cover several of the topics listed below among others:\n\rdata wrangling\rdata visualization\rdata imputation\rdata modeling\rtraditional statistics\rmachine learning\rmodel deployment\rdashboards\rautomation\r\rMost of the post I come across on these types of tasks use anything but HR type data that I can relate to. So, I’m hoping that if you have stopped by here, you can get a head start on applying some of the previously mentioned topics to your work.\n\rAbout me\rI got my BA in Psychology from the University of Georgia and then my Master’s and Ph.D. from the University of South Florida in Industrial-Organizational Psychology. In my work career, I’ve done a good deal of survey work along with validation studies, assessment creation, and both internal and external consulting.\nI’m also currently teaching advanced analytics in R to Master’s level students with a large emphasis on applicability (e.g. model interpretability, explainability, and return on investment). I want to give my students skills that are immediately applicable in the workplace.\n\rProgramming background\rVery little! I came from SPSS and after hitting one too many walls with what I was trying to do with my analysis, slowly started picking up R. I’m a loquacious coder who usually writes more comments than code, but that has saved my bacon 6 months to a year down the road on many occasions. I encourage you to comment on the different ways that you would have handled the particular task I’ve blogged about because on several occasions I have someone point out a new or different method that has saved me several hours of work.\n\r","date":1597968000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598034831,"objectID":"00b07d78a800c3903fe17388fe4c54e0","permalink":"http://robstilson.rbind.io/post/what-this-site-is-about/","publishdate":"2020-08-21T00:00:00Z","relpermalink":"/post/what-this-site-is-about/","section":"post","summary":"Welcome!\rWelcome to the blog! My main purpose in creating this site is to show those doing I-O Psychology and Human Resources (HR) type work, the way that I use R, Python, and occasionally Tableau in my work.","tags":[],"title":"What this site is about","type":"post"},{"authors":["Rob Stilson"],"categories":null,"content":" Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"http://robstilson.rbind.io/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic  Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click  PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions?  Ask\n Documentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"http://robstilson.rbind.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"http://robstilson.rbind.io/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"http://robstilson.rbind.io/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":["Rob Stilson","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"http://robstilson.rbind.io/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":["Rob Stilson","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"http://robstilson.rbind.io/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example conference paper","type":"publication"}]