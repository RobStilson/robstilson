---
title: 'Exploratory Data Analysis: Part 1'
author: Rob Stilson
date: '2020-08-21'
slug: exploratory-data-analysis-part-1
categories:
  - EDA
tags: []
subtitle: ''
summary: ''
authors: []
lastmod: '2020-08-21T14:51:39-04:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
output:
  blogdown::html_page:
    toc: true
---

# Summary

In this post I will introduce you to HR data from the IBM Watson Analytics Lab. The `modeldata` package contains this as the `attrition` data set. This data set is also available from [Kaggle](https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset). Here is the description from the *Details* portion of the documentation.

These data are from the IBM Watson Analytics Lab. The website describes the data with “Uncover the factors that lead to employee attrition and explore important questions such as ‘show me a breakdown of distance from home by job role and attrition’ or ‘compare average monthly income by education and attrition’. This is a fictional data set created by IBM data scientists.”. There are 1470 rows.

## Get Packages

This code chunk will load the necessary packages for the analysis. I've been using `pacman` for years and I encourage you to check it out as well. The `tryCatch` portion of the code will check your machine to see if `pacman` has already been installed. If it finds `pacman`, it will download it and if not it will move on. The `pacman::p_load` function is a wrapper for `library` and `require` and checks to see if the listed packages are installed. If they are not, it will attempt to install them from CRAN and/or any other repository in the `pacman` repository list (See *Description* under `?p_load` for additional information).

As a best practice, I also like to give a brief snippet of what each of the packages I'm loading will do in the analysis. This is helpful for anybody else who I send my code to, but also for myself if I need to come back to the code 6 months or more down the line.

```{r load packages, echo = FALSE, include= FALSE, warning = FALSE, message = FALSE}
#' <!-- ####################################################################################################### -->
#' <!-- ####################################################################################################### -->
#' <!-- ##################################LOADING PACKAGES##################################################### -->

tryCatch(require(pacman),finally=utils:::install.packages(pkgs='pacman',repos='http://cran.r-project.org'));
require(pacman)

# if the above doesn't work, use this code#
# tryCatch
# detach("package:pacman", unload = TRUE)
# install.packages("pacman", dependencies = TRUE)
# install.packages("pacman")

pacman::p_load(modeldata, # data sets useful for modeling packages
               dplyr, # a grammar of data manipulation
               tidyr, # tidy messy data
               readxl, # read Excel files
               ggplot2, # create elegant data visualisations using the grammar of graphics
               knitr, # a general-purpose package for dynamic report generation in R
               broom, # convert statistical objects into tidy tibbles
               purrr, # functional programming tools
               psych, #procedures for psychological, psychometric, and personality research
               conflicted, # an alternative conflict resolution strategy for like named function in different libraries
               janitor, # simple tools for examining and cleaning dirty data
               skimr, # compact and flexible summaries of data
               openxlsx, # read write and edit xlsx files
               tidyquant, # tidy quantitative financial analysis
               tidylog # logging for `dplyr` and `tidyr` functions
)

```

## Load packages

Here I'm explicitly calling the packages via `library`. Notice that `tidylog` is at the very end. This is necessary to load after `dplyr` so that all of the functions from `tidylog` work correctly. This is used in conjunction with the `conflicted` package. You can read more about it [here](https://github.com/elbersb/tidylog/blob/master/README.md).

```{r, echo = FALSE, include= FALSE, warning = FALSE, message = FALSE}

#' <!-- #Loading libraries -->

suppressPackageStartupMessages({
  library(modeldata)      
  library(dplyr)
  library(tidyr)
  library(readxl)
  library(ggplot2)
  library(knitr)
  library(broom)
  library(purrr)
  library(psych)
  library(conflicted)
  library(janitor)
  library(skimr)
  library(openxlsx)
  library(tidyquant)
  library(tidylog, warn.conflicts = FALSE)
})

for (f in getNamespaceExports("tidylog")) {
    conflicted::conflict_prefer(f, "tidylog", quiet = TRUE)
}

```

```{r}
conflict_prefer("filter", "dplyr")
```


## Bring in the data

For this demonstration we'll use `attrition` directly from the `modeldata` package. We'll rename it `Data` so we don't overwrite anything.


```{r}
library(modeldata)

data(attrition)

Data <- attrition
```

## Load data from Excel

If you downloaded the data set from Kaggle as an Excel file, you would used something like below. The easiest way to do this is probably to use File -> Import Data Set -> From Excel from the RStudio drop down menu.

```{r, eval = FALSE}
library(readxl)
Data <- read_excel("D:/Folder where the file is located/WA_Fn-UseC_-HR-Employee-Attrition.xlsx")
```

## Head

Ok, now that we've got our data into R, let's start looking around.

First, we'll use the `head` function to see the first few rows.

```{r}
head(Data)
```

 
If we wanted to see the first 10 rows instead of the first 6 rows we can change it up with `n`.

```{r}
head(Data, n = 10)
```

## Tail

We can also take a look at the bottom of the data with `tail`.

```{r}
tail(Data)
```

## HeadTail

And, if we would like to see both at once, we can use the `headTail` function from the `psych` package.

```{r}
headTail(Data)
```

If we wanted to use the `headTail` function without loading the `psych` library or if we want to make it explicit that we are using that function from that package, we could have done the following as well.

```{r, eval = FALSE}
psych::headTail(Data)
```

## Dimensions of the data

We can check out the dimensions of the data to get the number of rows and columns.

```{r}
dim(Data)
```

## Structure of the data

We can get the structure of the data.

```{r}
str(Data)
```

## Glimpse the data

A similar way to get this information is the `glimpse` function from `dplyr`. A benefit of `glimpse` is there doesn't seem to be a limit on the number of columns it can display where `str` seems to have a limit of the first 99.

```{r}
glimpse(Data)
```

## Skim the data

The `skimr` package is useful to get some additional information on the data frame (df) with the `skim` function. In addition to number of columns and rows, it gives you the following:

* column type with frequency
* any grouping variables
* each column with number missing
* if the column is ordered 
* number of unique occurrences in each column
* top counts in each column 
* mean
* standard deviation
* quantiles
* a mini histogram of the data

```{r}
skim(Data)
```

## Describe the data

We can lean on the `psych` package again with the `describe` function to get some information about the df.

```{r}
describe(Data)
```
## Save `describe` as a DF

If you want, you could save this to its own df so that you could then export it to Excel for a key stakeholder.

```{r}
Desc_Data <- as.data.frame(describe(Data))
```

## Round `describe` and save as a DF

That's pretty gnarly. Let's see if we can round that to 3 decimal places.

```{r}
Desc_Data <- round(as.data.frame(describe(Data)),3)
```

Much better!

## Write to Excel

To send it to an Excel file we simply call upon `openxlsx` with the `write.xlsx` function.

```{r, eval = FALSE}
openxlsx::write.xlsx(Desc_Data, "00_Data/Descriptive Statistics for Data.xlsx")
```

## Add an ID column

If we want to add and ID column, we can use the `mutate` function from `dplyr` to create a new variable. Here we will create a new object called `TEST` just in case this doesn't go according to plan.

```{r}
TEST <- Data %>%
  mutate(ID = row_number())
```

And we'll take a look at our new DF.

```{r}
head(TEST)
```

It has added it to the very end. We would like to put this as the first column. There are a few ways to do this. Using the `select` function, we can name the `ID` column we just created and use the `everything` function to tell it to put `ID` first and then put `everything` after `ID`.

```{r}
TEST <- TEST %>%
  select(ID, everything())

head(TEST)
```

We could have also used the new `relocate` function from `dplyr` to do the same thing and tell it to put `ID` before `Age`.

```{r}
TEST <- TEST %>%
  relocate(ID, .before = Age)

head(TEST)
```

Now that we know this is going to work we can simply save `Data` as the updated `Data` object with our new `ID` variable located before `Age` like so.

```{r}
Data <- Data %>% 
    mutate(ID = row_number()) %>%
  relocate(ID, .before = Age)

head(Data)
```

Notice we did both of those steps together. To do this we used a pipe, `%>%`. Pipes are very useful for string steps of a code together in an easy to read and easy to follow fashion. I typically pronounce `%>%` as "then". So, we mutated our new `ID` varable to correspond to row number *then* we relocated our `ID` variable to be before `Age`.

# Character Data

Now we will get to know the character variables a little bit better. 

I learned the following methodology from Matt Dancho's (@mdancho84) data science class at [https://www.business-science.io/](https://www.business-science.io/). If you are interested in applying R to business problems, I highly recommend you check his classes out.

We're now going to use some more of our `dplyr` verbs. First up is `select_if`. This makes it easier to select certain types of data. In this case, it will be character data. We can pair it with `is.character` and pipe in `glimpse`.

```{r}
Data %>%
  select_if(is.character) %>%
  glimpse()
```

If you've enabled `tidylog`, you see that `select_if: dropped all variables`. I forgot that the built in version of the `Attrition` data had already been factorized. Let's try again and swap out `is.factor` for `is.character`.

```{r}
Data %>%
  select_if(is.factor) %>%
  glimpse()
```

Much better! We can now see more information about the 15 columns it returned. 

Now let us take a look at the unique variables within each column. We will call up the `map` feature from `purrr`.

```{r}
Data %>%
    select_if(is.factor) %>%
    map(unique) #from purrr
```

Since these are factors, in addition to the unique values, it also gives us the levels. Notice `WorkLifeBalance` at the very bottom and the levels of Bad < Good < Better < Best. This is showing that Bad is less than Good which is less than Better and so on. We'll revisit how to do this in a later post as the data almost never hits your inbox  with this information included.

Another way to look at the data is to use the `table` function. We will swap out `unique` for `table` within `map`.

```{r}
Data %>%
    select_if(is.factor) %>%
    map(table)
```

This way of looking at the data is very helpful since you can begin to get a feel for the factors. `Performance` rating has 0 for both `Low` and `Good`. This may be accurate or it may be some faulty data. Seeing this let's you reach out to whoever would know and either check off on the data or realize a mistake has been made before you go further in your analysis. 

You can also get proportions if that helps you to understand the data better. I won't go too much into how this works here, but we may step through this a bit more in a future post if you are interested in doing so.

```{r}
# To get proportions
Data %>%
    select_if(is.factor) %>%
    map(~ table(.) %>% prop.table()) #anonymous function
```

This now gives us the proportions, but I struggle with anything more than about 2 or 3 decimals. Let's go ahead and clean this up a bit. We have wrapped everything in the `round` function. Also, notice that I didn't always know how to do this. I've put where I learned how do to this as a URL I've commented out from [stackoverflow.com](stackoverflow.com). Remember to leave yourself breadcrumbs to find your way home! You may not need these breadcrumbs tomorrow, but you will thank yourself when you have to come back to your code 6 months or more down the road. I also think the breadcrumbs are helpful if you have to share your code. 

```{r}
# To get proportions
# Rounded (From: https://stackoverflow.com/questions/43013016/how-to-create-multiple-frequency-tables-with-percentages-across-factor-variables)
Data %>%
    select_if(is.factor) %>%
    map(~ round(table(.) %>% prop.table(), 2)) #anonymous function
```

Rounded to 2 decimal places looks much better to my eye. If 3 looks better to yours, simply swap out the 2 for a 3.

```{r}
# To get proportions
# Rounded (From: https://stackoverflow.com/questions/43013016/how-to-create-multiple-frequency-tables-with-percentages-across-factor-variables)
Data %>%
    select_if(is.factor) %>%
    map(~ round(table(.) %>% prop.table(), 3)) #anonymous function
```

Remember, there are also usually multiple ways of doing things in R. We could have accomplished the same thing with the following code. I say choose whichever coding style looks better to you.


```{r}
# To get proportions
# Rounded (From: https://stackoverflow.com/questions/43013016/how-to-create-multiple-frequency-tables-with-percentages-across-factor-variables)
Data %>%
    select_if(is.factor) %>%
    map(~round(prop.table(table(.x)),2))
          
```

We could have also utilized the `tabyl` function from `janitor`, but now I'm going down a bit of a rabbit hole. The point is, mess around with the code and the data. Make it your own.

```{r}
Data %>%
    select_if(is.factor) %>%
    map(tabyl)
```

```{r}
Data %>%
    select_if(is.factor) %>%
    map(tabyl)

# Data %>%
#     select_if(is.factor) %>%
#     map(~ round(table(.) %>% prop.table(), 3)) #anonymous function
```

# Numeric Data

I also enjoy Matt Dancho's way of looking at numeric data so I wanted to include this. The code below should look pretty familiar. We are just swapping out `is.numeric` for `is.factor`. Adding `length` simply tells us how many unique values are in each numeric variable. So, as expected, we see 1470 for `ID` which makes sense because we have 1470 rows. 

```{r}
Data %>%
    select_if(is.numeric) %>%
    map(~ unique(.) %>% length())
```

Changing out `map` for `map_df` will turn this into a data frame. That makes it easier to export the file to something like Excel.

```{r}
Data %>%
    select_if(is.numeric) %>%
    map_df(~ unique(.) %>% length()) # tries to turn it into a df instead of a list
```

Throwing `gather` after the pipe will make the data tall or pivot it.

```{r}
Data %>%
    select_if(is.numeric) %>%
    map_df(~ unique(.) %>% length()) %>% # tries to turn it into a df instead of a list
    gather()
```
We can pipe in `arrange` and set it to `desc` for descending to get the values from most to least.

```{r}
Data %>%
    select_if(is.numeric) %>%
    map_df(~ unique(.) %>% length()) %>% # tries to turn it into a df instead of a list
    gather() %>%
    arrange(desc(value)) # Move the largest value to the top and go descending 
```

Now it would be super easy to save it as a new object like `Values_for_Excel` or something like that to export with `openxlsx::write.xlsx`. We'll cover that more in a later post.

```{r}
Values_for_Excel <- Data %>%
    select_if(is.numeric) %>%
    map_df(~ unique(.) %>% length()) %>% # tries to turn it into a df instead of a list
    gather() %>%
    arrange(desc(value)) # Move the largest value to the top and go descending 
```

`Values_for_Excel` now appears as a new df in your Global Environment.

A neat trick I learned from Matt Dancho's class is the following for determining if a numeric variables is more likely continuous or discrete.

Values higher than 10 are probably continuous. `PercentSalaryHike`, `YearsSinceLastPromotion`, etc. 
```{r}
Data %>%
    select_if(is.numeric) %>%
    map_df(~ unique(.) %>% length()) %>% # tries to turn it into a df instead of a list
    gather() %>%
    arrange(value) %>%
    filter(value > 10) #probably continuous if more than 10
```

Values less than 10 are probably discrete. `JobLevel`, `NumCompaniesWorked`, etc. 

```{r}
Data %>%
    select_if(is.numeric) %>%
    map_df(~ unique(.) %>% length()) %>% # tries to turn it into a df instead of a list
    gather() %>%
    arrange(value) %>%
    filter(value <= 10) #probably discrete if less than 10
```

You may need to change 10 to be a little lower or higher to differentiate your specific data, but it is a great place to start. 

# Wrap up

That is it for now. Hopefully you have found something you can use here. Keep coming back as we will start to visualize the data and add to it as we try to make it more and more like something you will encounter in your work place. 

# Just the code

Thanks to [Yihui Xie](https://yihui.org/) and [Lucy D'Agostino](https://www.lucymcgowan.com/), I stumbled across this bit of [sorcery](https://yihui.org/en/2018/09/code-appendix/) using `PURL` where you simply include an `R` code chunk to save all of the above code in the appendix.

```{r ref.label = knitr::all_labels(), echo = TRUE, eval = FALSE}

```
