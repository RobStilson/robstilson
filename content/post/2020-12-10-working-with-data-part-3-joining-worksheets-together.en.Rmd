---
title: Working with Data-Part 3-Joining worksheets together
author: robstilson
date: '2020-12-09'
slug: working-with-data-part-3-joining-worksheets-together
categories:
  - Joins
tags: []
subtitle: ''
summary: ''
authors: []
lastmod: '2020-12-09T19:33:20-05:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
output:
  blogdown::html_page:
    toc: true
---

```{r load packages, echo = FALSE, include= FALSE, warning = FALSE, message = FALSE}
#' <!-- ####################################################################################################### -->
#' <!-- ####################################################################################################### -->
#' <!-- ##################################LOADING PACKAGES##################################################### -->

tryCatch(require(pacman),finally=utils:::install.packages(pkgs='pacman',repos='http://cran.r-project.org'));
require(pacman)

# if the above doesn't work, use this code#
# tryCatch
# detach("package:pacman", unload = TRUE)
# install.packages("pacman", dependencies = TRUE)
# install.packages("pacman")

pacman::p_load(modeldata, # data sets useful for modeling packages
               dplyr, # a grammar of data manipulation
               tidyr, # tidy messy data
               readxl, # read Excel files
               ggplot2, # create elegant data visualisations using the grammar of graphics
               knitr, # a general-purpose package for dynamic report generation in R
               broom, # convert statistical objects into tidy tibbles
               purrr, # functional programming tools
               psych, #procedures for psychological, psychometric, and personality research
               conflicted, # an alternative conflict resolution strategy for like named function in different libraries
               janitor, # simple tools for examining and cleaning dirty data
               skimr, # compact and flexible summaries of data
               openxlsx, # read write and edit xlsx files
               RCurl, # general Network (HTTP/FTP/...) Client Interface for R
               tidyquant, # tidy quantitative financial analysis
               tidylog # logging for `dplyr` and `tidyr` functions
)

```

```{r, echo = FALSE, include= FALSE, warning = FALSE, message = FALSE}

#' <!-- #Loading libraries -->

suppressPackageStartupMessages({
  library(modeldata)      
  library(dplyr)
  library(tidyr)
  library(readxl)
  library(ggplot2)
  library(knitr)
  library(broom)
  library(purrr)
  library(psych)
  library(conflicted)
  library(janitor)
  library(skimr)
  library(openxlsx)
  library(RCurl)
  library(tidyquant)
  library(tidylog, warn.conflicts = FALSE)
})

for (f in getNamespaceExports("tidylog")) {
    conflicted::conflict_prefer(f, "tidylog", quiet = TRUE)
}

```

```{r, echo = FALSE, include= FALSE, warning = FALSE, message = FALSE}
conflict_prefer("filter", "dplyr")
```

# Summary

In the [Part 2](http://robstilson.rbind.io/post/working-with-data-part-2-pulling-in-data-from-excel/), we covered loading Excel data from you local machine into R. Now we will cover how to join all of that data together into one worksheet.

## Bring in the data

If you haven't look at [Part 2](http://robstilson.rbind.io/post/working-with-data-part-2-pulling-in-data-from-excel/) yet, please go there now in order to get the various worksheets we are going to use directly onto your machine.

Now we will load the data from our desktop. You will need to change the directory to reflect where you stored the file. We will call the data `df`, which is short for data frame.

Remember the sheets were:

* accident
* fair_pay
* hr         
* hr_2        
* performance 
* recruitment 
* survey      
* survey_2 

We will lump all the code necessary to get the worksheets onto our machine together and then get to joining them together.

```{r}
xl_data <- "D:/Blog/Additional Data for Attrition Data Set_09_09_2020.xlsx"

tab_names <- excel_sheets(path = xl_data)

list_all <- lapply(tab_names, function(x) read_excel(path = xl_data, sheet = x))

names(list_all) <- tab_names

list2env(list_all, .GlobalEnv)
```

In your Global Environment, you should see the following:

* 8 dfs that we just uploaded

* 1 "List of 8"

* 2 Values
    - tab_names
    - xl_data
    
We're going to join all of the data from the 8 worksheets together into one larger worksheet that we will call `df`.

## Determining what columns on which to join

Originally, I thought this was going to be easy as I assumed (and we know what that does...) that across all 8 dfs, we had a variable that was common to all of them, but that turned out to not be the case. 

So, we will work backwards and I'll tell you where we end up.

I found [this](https://stackoverflow.com/questions/52860105/how-to-find-common-variables-in-different-data-frames) excellent answer on Stackoverflow and wanted to include it in this write up as I think you may find additional uses for it. 

The original question was, "how do I find the common column (or columns) across all of my dfs of interest so that I can then use those to join all of my data together?" It turns out this only works if you actually have a common column across all of your dfs. If `employee_id` occurs in 99 out of 100 of the files you are trying to merge together, you are now in trouble, because the solutions I found won't return anything it the column isn't present in all 100 (or however many) dfs you have.

But I found a bit of a work around. If we take the original solution to the question asked, we will assign our `list_all` that we used to get the tab names from our worksheet and pull all of the data in to `L` (but it could be any letter or name you choose).

Then, we will combine the following:

* `table` - `table` uses the cross-classifying factors to build a contingency table of the counts at each combination of factor levels.

* `unlist` - Given a list structure x, `unlist` simplifies it to produce a vector which contains all the atomic components which occur in x.

* `lapply` - `lapply` returns a list of the same length as X, each element of which is the result of applying FUN to the corresponding element of X.

* `names` - Functions to get or set the names of an object.

So the code `tab <- table(unlist(lapply(L, names)))` is doing the following:

1. Looks for the columns names within `L` or whatever you assigned `list_all` to

2. `unlist` all of them via `lapply`

3. Creates a `table` of the column names across all of the dfs in `L` and tallies up how many times they occurred

4. Assigns them to the object `tab`

Then, the final line of code, `names(tab[tab == length(L)])` looks for any `names` in the table `tab` that occur the same amount of times as the `length` of `L`, your list, which in this instance is 8.

If you are unfamiliar with any of the R functions above, I implore you to learn more about them as you will probably see them a lot.

Alright, what actually happens if we run the code?

```{r}
#From: https://stackoverflow.com/questions/52860105/how-to-find-common-variables-in-different-data-frames
L <- list_all
tab <- table(unlist(lapply(L, names)))
names(tab[tab == length(L)])
```

`character(0)`. What does that mean? That means that no columns occurred across all of the dfs. You may be feeling despondent at this point, but don't give up hope! There is magic in that second line `tab <- table(unlist(lapply(L, names)))` as it will tell us which columns occur and how many times it happens.

Let's run the code again, but just the `table(unlist(lapply(L, names)))` portion to see what we get.

```{r}
table(unlist(lapply(L, names)))
```

  <!--     accident_type           attrition          department         employee_id          engagement              gender  -->
  <!--                 1                   1                   3                   7                   2                   1  -->
  <!--         job_level            location            new_hire      overtime_hours  performance_rating              rating  -->
  <!--                 2                   1                   1                   1                   1                   1  -->
  <!-- recruiting_source              salary     sales_quota_pct vacation_days_taken                year  -->
  <!--                 1                   2                   1                   1                   3  -->
  
This is very useful information! We can see that `accident_type` showed up once across all 8 dfs, `salary` showed up twice and so forth, but the gem is that `employee_id` showed up 7 times. It is also great that `employee_id` happens to be a unique identifier so that is what we will use to merge 7 out of our 8 dfs together. 

Here is where we jump the shark...I was wondering, well if I had 100 dfs and only 99 of them had the required column for merging...how would I know which one didn't. The brute force way is to simply go through each and every one of them and check, but that is not scalable nor conducive to a mistake free process. After searching a bit, I came across (
[this solution](https://stackoverflow.com/questions/32251679/extracting-data-frames-from-a-list-based-on-column-names-in-r) on Stackoverflow. We create a function to take a look into all the dfs in our list, `list_all` and return a logical if `employee_id` is or is not present. Notice that for `recruitment`, it list `FALSE`. Sure enough, if you look at the `recruitment` df, it does not contain our desired column.

```{r}
#From: https://stackoverflow.com/questions/32251679/extracting-data-frames-from-a-list-based-on-column-names-in-r
reqnames <- c("employee_id")
lapply(list_all, function(x) any(names(x) %in% reqnames))
```
If we count, we see that `recruitment` is the 6th df in the list. We can create a new list called `new_list` where we remove `recruitement` and then we can run it through our code from above.

```{r}
new_list <- list_all[-6]
```

```{r}
L <- new_list
tab <- table(unlist(lapply(L, names)))
names(tab[tab == length(L)])
```

It now returns `employee_id`!

Ok, now that we have gone through all of that, let's join the 7 sheets that contain `employee_id` together. Again, we could do this one at a time, but there is a much easier and quicker way. Will will call this df `Extra_Data` since we are going to join it with our main attrition data.


```{r}
Extra_Data <- list(accident, #year
fair_pay,
hr,         
hr_2, #year       
performance, 
survey,      
survey_2) %>% # year
  reduce(left_join, by = c("employee_id"))
```

Now if we take a look at the new df, we see that some columns were duplicated and recieved a `.x` or `.y`. This is due to R not being able to handle multiple columns in the same df with the same name (which makes sense). There may be a more straigth forward way to deal with this, but we are simply going to look at all the column names and then only select the original ones and then rename them to remove the suffix.

```{r}
head(Extra_Data)
```
```{r}
cat(colnames(Extra_Data), sep = ",\n")
```

```{r}
Extra_Data <- Extra_Data %>%
  select(year.x,
         employee_id,
         accident_type,
         department.x,
         salary.x,
         new_hire,
         job_level.x,
         # department.y,
         # job_level.y,
         gender,
         # year.y,
         location,
         overtime_hours,
         rating,
         department,
         engagement.x,
         # salary.y,
         vacation_days_taken,
         year
         # engagement.y
         )
```

Notice in this code that everything with the suffix `.y` has gotten the CTRL + SHIFT + c treatment and been commented out.

```{r}
head(Extra_Data)
```

And now we are still having a problem with `department` and it looks like `year` may have gotten mixed up as well. We may have to do this in layers after all.

First we will gather all of the extra data that contains `year`.

```{r}
Extra_Data_year <- list(accident, #year
hr_2, #year       
survey_2) %>% # year
  reduce(full_join, by = c("employee_id", "year")) #Notice we are doing a full_join instead of a left_join since we want to get all of the data and we know that someone may have had an accident one year but not another year.
```

```{r}
head(Extra_Data_year)
```
That is looking clean. Now we will use all of the data that has `department`.

```{r}
Extra_Data_dept <- list(fair_pay,
hr,         
survey) %>% 
  reduce(left_join, by = c("employee_id", "department"))
```

```{r}
head(Extra_Data_dept)
```
We've got some duplicates here. Let's just make sure they are the same before we eliminate one.

```{r}
Extra_Data_dupe <- Extra_Data_dept %>%
  mutate(sal_same = salary.x == salary.y, #This will create a logical TRUE/FALSE We hope to see all TRUE for both
         job_same = job_level.x == job_level.y) %>%
  select(employee_id,
         sal_same,
         job_same)

summary(Extra_Data_dupe)
```

The summary is telling us that we have 16 `FALSE` where they don't match. Let's find them and take a look.

```{r}
sal_false <- Extra_Data_dupe %>%
  filter(sal_same == FALSE)
```

Now let's filter `Extra_Data_dept` using `sal_false` to bring out the 16 we want to take a look at.

```{r}
sal_look <- Extra_Data_dept %>%
  filter(employee_id %in% sal_false$employee_id)
```

They are definitely different. In this case, we will default to the value given in the `fair_pay` data, but in a real situation we would reach out to the appropriate SME in order to determine the correct course of action. If for whatever reason, that is not possible, remember, document, document, document!!!! This will help future you and anybody else who encounters your code.

```{r}
Extra_Data_dept <- Extra_Data_dept %>%
  select(employee_id,
         department,
         salary.x, # Using salary.x instead of .y due to inclusion in `fair_pay` data, but check on this
         new_hire,
         job_level.x,
         # job_level.y,
         gender,
         engagement,
         # salary.y,
         vacation_days_taken) %>%
  rename(salary = salary.x,
         job_level = job_level.x)

colnames(Extra_Data_dept)
```
The names look good now.

We'll merge together `Extra_Data_year` with `Extra_Data_dept`.

Before we join `Extra_Data_year`, let's make it wide_data instead of tall_data with `pivot_wider`.

```{r}
Extra_Data_year_cast <- Extra_Data_year %>%
  pivot_wider(names_from = year,
              values_from = c(accident_type,
                              overtime_hours,
                              engagement))
```


```{r}
Extra_Data_year_dept <- Extra_Data_dept %>%
  left_join(Extra_Data_year_cast, by = "employee_id")
```

Finally, we will join the `Extra_Data_year_dept` data with `performance` before the final join with the main data.

```{r}
Extra_Data <- Extra_Data_year_dept %>%
  left_join(performance, by = "employee_id")
```
Load the original data.

```{r}
library(modeldata)

data(attrition)

Data <- attrition
```

```{r}
library(readr)
Data <- read_csv("D:/Blog/WA_Fn-UseC_-HR-Employee-Attrition.csv")
```


`left_join` with the `Extra_Data` we just created.

```{r}
Data <- Data %>%
  left_join(Extra_Data, by = c("EmployeeNumber" = "employee_id"))
```


# Wrap up
We consolidated all of the extra data frames into one and then merged them with our original dataframe.

# Just the code


```{r ref.label = knitr::all_labels(), echo = TRUE, eval = FALSE}

```
